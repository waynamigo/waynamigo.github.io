<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>No title | waynamigo&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content>
    <meta name="description" content="Abstract In this paper, we present an effective transformer- based framework for visual grounding, to address the task of grounding a language query to the corresponding region onto an image. The stat">
<meta property="og:type" content="article">
<meta property="og:title" content="No title">
<meta property="og:url" content="http://waynamigo.github.io/2023/01/08/paper/index.html">
<meta property="og:site_name" content="waynamigo&#39;s blog">
<meta property="og:description" content="Abstract In this paper, we present an effective transformer- based framework for visual grounding, to address the task of grounding a language query to the corresponding region onto an image. The stat">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2023-01-08T12:10:07.649Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="No title">
<meta name="twitter:description" content="Abstract In this paper, we present an effective transformer- based framework for visual grounding, to address the task of grounding a language query to the corresponding region onto an image. The stat">
    
        <link rel="alternate" type="application/atom+xml" title="waynamigo&#39;s blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.png">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">waynamigo</h5>
          <a href="mailto:waynamigo@gmail.com" title="waynamigo@gmail.com" class="mail">waynamigo@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Homepage
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/waynamigo" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis"></div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title"></h1>
        <h5 class="subtitle">
            
                <time datetime="2023-01-08T11:22:11.951Z" itemprop="datePublished" class="page-time">
  2023-01-08
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Abstract"><span class="post-toc-number">1.</span> <span class="post-toc-text">Abstract</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">2.</span> <span class="post-toc-text">Introduction</span></a></li></ol>
        </nav>
    </aside>


<article id="post-paper"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title"></h1>
        <div class="post-meta">
            <time class="post-time" title="2023-01-08 19:22:11" datetime="2023-01-08T11:22:11.951Z"  itemprop="datePublished">2023-01-08</time>

            


            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol>
<li><p>In this paper, we present an effective transformer- based framework for visual grounding, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. How- ever, the involvement of certain mechanisms in fusion module design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with spe- cific scenarios, and limits the plenitudinous interaction be- tween the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher per- formance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid mak- ing predictions out of a set of candidates (i.e., region pro- posals or anchor boxes). Extensive experiments are con- ducted on five widely used datasets, and a series of state- of-the-art records are set by our TransVG.<br>在本文中，我们提出了一个简洁而有效的基于变换器的视觉定位框架，以解决将语言查询与图像上的相应区域进行定位的任务。最先进的方法，包括两阶段或单阶段的方法，都依赖于一个复杂的模块，由人工设计的机制来执行查询推理和多模式融合。然而，某些机制在融合模块设计中的参与，如查询分解和图像场景图，使得这些模型很容易过度适应具有特殊场景的数据集，并限制了视觉-语言环境之间的丰富互动。为了避免这一问题，我们建议利用变换器建立多模态对应关系，并通过经验表明，复杂的融合模块（如模块化注意力网络、动态图和多模态树）可以被简单的变换器编码器层所取代，并具有更高的性能。此外，我们将视觉接地重新表述为一个直接的坐标回归问题，并避免从一组候选人（即区域提案或锚箱）中进行预测。在五个广泛使用的数据集上进行了广泛的实验，我们的TransVG创造了一系列先进的记录。</p>
</li>
<li><p>In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.</p>
</li>
</ol>
<p>在这项工作中，我们探索了简洁而有效的基于Transformer的视觉定位框架。以前的方法一般都是用人工设计的机制来解决视觉基础的核心问题，即多模态融合和推理。这种启发式的设计不仅复杂，而且使模型容易过度适应特定的数据分布。为了避免这种情况，我们首先提出了TransVG，它通过Transformers建立多模态对应关系，并通过直接回归箱体坐标对参考区域进行定位。我们的经验表明，复杂的融合模块可以被一个简单的变形器编码器层堆栈所取代，而且性能更高。然而，TransVG中的核心融合Transformer是独立于单模态编码器的，因此应该在有限的视觉接地数据上从头开始训练，这使得它很难被优化并导致次优的性能。为此，我们进一步引入TransVG++，进行了两方面的改进。首先，我们通过利用视觉变换器（ViT）进行视觉特征编码，将我们的框架升级为一个纯粹的基于变换器的框架。另一方面，我们设计了语言条件下的视觉转化器，删除了外部融合模块，在中间层重新使用单模态的ViT进行视觉-语言融合。我们在五个流行的数据集上进行了广泛的实验，并报告了一系列的最先进的记录。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ol>
<li><p>Visual grounding (also known as referring expression comprehension [31, 60], phrase localization [23, 38], and natural language object retrieval [21, 25]) aims to predict the location of a region referred by the language expression onto an image. The evolution of this technique is of great potential to provide an intelligent interface for the natural language expression of human beings and the visual com- ponents of the physical world. Existing methods addressing this task can be broadly grouped into the two-stage and one- stage pipelines shown in Figure 1. In specific, the two-stage approaches [31, 34, 46, 60] first generate a set of sparse re- gion proposals and then exploit region-expression matching to find the best one. The one-stage approaches [9, 27, 56] perform visual-linguistic fusion at intermediate layers of an object detector and output the box with the maximal score over pre-defined dense anchors.<br>Multi-modal fusion and reasoning is widely studied in the literature [1, 35, 49, 54, 65], and it is the core problem in visual grounding. In general, the early two-stage and one-stage methods address multi-modal fusion in a simple way. Concretely, the two-stage Similarity Net [46] measures the similarity between region and expression embedding with an MLP, and the one-stage FAOA [56] encodes the language vector to visual feature by direct concatenation. These sim- ple designs are efficient but lead to sub-optimal results, es- pecially on long and complex language expressions. Fol- lowing studies have proposed diverse architectures to im- prove the performance. Among two-stage methods, mod- ular attention network [59], various graphs [48, 52, 53], and multi-modal tree [28] are designed to better model the multi-modal relationships. The one-stage method [55] has also explored better query modeling by proposing a multi- round fusion module.<br>Despite the effectiveness, these complicated fusion mod- ules are built on certain pre-defined structures of language queries or image scenes, inspired by the human prior. Typi- cally, the involvement of manually-designed mechanisms in fusion module makes the models overfit to specific scenar- ios, such as certain query lengths and query relationships, and limits the plenitudinous interaction between visual- linguistic contexts. Moreover, even though the ultimate goal of visual grounding is to localize the referred object, most of the previous methods ground the queried object in an indirect fashion. They generally define surrogate prob- lems of language-guided candidates prediction, selection, and refinement. Typically, the candidates are sparse region proposals [60, 31, 46] or dense anchors [56], from which the best region is selected and refined to get the final grounding box. Since these methods’ predictions are made out of can- didates, the performance is easily influenced by the prior knowledge to generate proposals (or pre-defined anchors) and by the heuristics to assign targets to candidates.<br>In this study, we explore an alternative approach to avoid the aforementioned problems. Formally, we introduce a neat and novel transformer-based framework, namely TransVG, to effectively address the task of visual ground- ing. We empirically show that the structurized fusion mod- ules can be replaced by a simple stack of transformer en- coder layers. Particularly, the core component of transform- ers (i.e., attention layer) is ready to establish intra-modality and inter-modality correspondence across visual and lin- guistic inputs, despite that we do not pre-define any specific fusion mechanism. Besides, we find that directly regressing the box coordinates works better than previous methods to ground the queried object indirectly. Our TransVG directly outputs 4-dim coordinates to ground the object instead of making predictions based on a set of candidate boxes.</p>
</li>
<li><p>VISUAL grounding, which aims to localize a region re- ferred to by a language expression in an image, is a core technology to bridge the natural language expression deliv- ered by human beings and visual contents in the physical world. The evolution of this eachnique is of great potential to promote vision-language understanding, and to provide an intelligent interface for human-machine interaction. Ex- isting methods addressing this task generally follow two- stage or one-stage pipelines shown in Figure 1. Specifically, two-stage approaches [1], [2] first generate a set of region proposals, and then take visual grounding as a natural language object retrieval problem [3], [4] to find the best matching region given language expressions. Differently, one-stage approaches [5], [6], [7] perform vision-language fusion at the output of the vision backbone network and the language model. Then, they make dense predictions with a sliding window over pre-defined anchor boxes, and keep the box with the maximum score as the final prediction.<br>Multi-modal fusion and reasoning is the primary prob- lem in visual grounding. The early two-stage and one-stage methods address multi-modal fusion in a simple way. Con- cretely, the pioneer two-stage method, Embedding Net [9],measures the similarity between region embedding and expression embedding by cosine distance. The early one- stage approach, FAOA [7], encodes the language feature vector to each spatial position of vision feature maps by directly concatenating them. In general, these attempts are efficient, but lead to sub-optimal results, especially when it comes to complicated language expressions [10], [11]. Following studies have proposed diverse architectures to ameliorate their performance. Among two-stage methods, modular attention network [12], various scene graphs [13], [14], [15], and multi-modal tree [10], [16] are designed to improve multi-modal relationships modeling. The one- stage method [11] also explores query decomposition and proposes a multi-round fusion mechanism.<br>Despite their effectiveness, these sophisticated fusion or matching modules are built on pre-assumed dependencies of language expressions and visual scenes, making the mod- els easily overfit specific scenarios, such as certain query lengths or objects relationships. Meanwhile, these mecha- nisms limit the plenitudinous interaction between vision and language contexts, which also hurts the performance of visual grounding algorithms. Besides, even though the target is to localize the referred region, most of the previ- ous methods achieve this target in an indirect way. They generally define surrogate problems of language-guided candidates matching, selection, and refinement, following the common practice of image-text retrieval and object detection. Therefore, extra efforts have to be devoted to obtain candidates, including region proposals [2], [17], [18] and pre-defined anchor boxes [7], [11]. Since these methods’ predictions are made out of candidates, their performance is easily influenced by the step to generate such candidates and by the heuristics to assign targets to candidates.</p>
</li>
</ol>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2023-01-08T12:10:07.649Z" itemprop="dateUpdated">2023-01-08 20:10:07</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="https://waynamigo.cn">
            <img src="/img/avatar.jpg" alt="waynamigo">
            waynamigo
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://waynamigo.github.io/2023/01/08/paper/&title=waynamigo's blog&pic=http://waynamigo.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://waynamigo.github.io/2023/01/08/paper/&title=waynamigo's blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://waynamigo.github.io/2023/01/08/paper/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=waynamigo's blog&url=http://waynamigo.github.io/2023/01/08/paper/&via=http://waynamigo.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://waynamigo.github.io/2023/01/08/paper/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2023/01/01/2023-01-01-kubernets_tutorial(Overview)/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Kubernetes Tutorial and Implementation(Overview)</h4>
      </a>
    </div>
  
</nav>



    

















<section class="comments" id="comments">
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
        var id = location.pathname
        if (location.pathname.length > 50) {
          id = location.pathname.replace(/\/\d+\/\d+\/\d+\//, '').replace('/', '').substring(0, 50)
        }
        const gitalk = new Gitalk({
          clientID: '1c48e09d4abbbe0f86a1',
          clientSecret: 'd42e38dee9898d2c2a362f9feac360efdd5e8e41',
          repo: 'waynamigo.github.io',
          owner: 'waynamigo',
          admin: ['waynamigo'],
          id: id,      // Ensure uniqueness and length less than 50
          title: document.title.split('|')[0],
          distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</section>




</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        

        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>waynamigo &copy; 2018 - 2023</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">鲁ICP备18055379号</a><br>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://waynamigo.github.io/2023/01/08/paper/&title=《No title》 — waynamigo's blog&pic=http://waynamigo.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://waynamigo.github.io/2023/01/08/paper/&title=《No title》 — waynamigo's blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://waynamigo.github.io/2023/01/08/paper/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《No title》 — waynamigo's blog&url=http://waynamigo.github.io/2023/01/08/paper/&via=http://waynamigo.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://waynamigo.github.io/2023/01/08/paper/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLElEQVR42u3aO3LDMAwFQN//0krrwlYeALogtawyHoXiOjMMfq9XvK639f7J5Plvz9zvPFoYGBjbMq7bdf9MsnOyQ/W9H96OgYHxAEZyiPxAySWbf333Z8PAwMBIAsFknzzgw8DAwFjFSFLQXqiHgYGBMUli88Dxfv/k7T/MxTEwMDZkTMK1X//8k/4GBgbGVoyruO6Ds/l1ebUWBgbG2YxqKa2ZXhaboL3zYGBgnMrIjzUfrei1AaK3Y2BgHM2YF87yYlkvfEwGPjAwMJ7J6IVxa/fJ010MDIwnMFYlmfMLNzlosx+LgYGxLWPV6EO1w7g4GcbAwHgYY94q6AV2vULb1wsXAwPjIMakWJ8fKyrrt6r9GBgYT2BUrdVRsN4R87C1+SfDwMDYipE8lIdo1RS3Wnor3NAYGBgPYFQjyt54Wd7+LJwHAwPjaEZydVYHyPL9m9l27z8ABgbGVoxqUWxS9J+ntV8/x8DAOJoxSW4nRbTkwr3fs4zBwMDYnLGqoDYZsOiFoR+6HBgYGEczeiW2aiJ6X/rPE+MF/VgMDIxtGdXyWS+kqzY7o5NgYGAcyriKK0lNq6lv/rtRbIuBgXEcY1VrMwn7epf74j4tBgbGtoxVoxW9MlwvPR590xgYGNsyes3I3lF+gcTAwMDIy/S9Fma1CPjPjBsGBsbjGVVStTxXLclhYGA8hzEf+SoMRhRT6MXlNgwMjA0Zo0mNOEycXK+FABEDA+M0xh8xnyLgPrhp+QAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>






<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'mole';
            clearTimeout(titleTime);
        } else {
            document.title = 'mole';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"live2d-widget-model-koharu"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
