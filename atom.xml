<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>waynamigo&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://waynamigo.github.io/"/>
  <updated>2023-01-08T11:22:03.017Z</updated>
  <id>http://waynamigo.github.io/</id>
  
  <author>
    <name>waynamigo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes Tutorial and Implementation(Overview)</title>
    <link href="http://waynamigo.github.io/2023/01/01/2023-01-01-kubernets_tutorial(Overview)/"/>
    <id>http://waynamigo.github.io/2023/01/01/2023-01-01-kubernets_tutorial(Overview)/</id>
    <published>2022-12-31T16:00:00.000Z</published>
    <updated>2023-01-08T11:22:03.017Z</updated>
    
    <content type="html"><![CDATA[<p>本篇为OverView，内容包括kubectl的基础操作，整理的知识框架基于kubernetes官方文档<a href="https://kubernetes.io/docs/tutorials/" target="_blank" rel="noopener">v1.26</a>， 元旦期间系统整理一下。</p><a id="more"></a><h2 id="Kubernetes-Components-and-Architecture"><a href="#Kubernetes-Components-and-Architecture" class="headerlink" title="Kubernetes Components and Architecture"></a>Kubernetes Components and Architecture</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/components-of-kubernetes.svg" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="I-Control-Plane-Components"><a href="#I-Control-Plane-Components" class="headerlink" title="I. Control Plane Components"></a>I. Control Plane Components</h3><p>*<red>Control Plane</red>是对集群进行调度管理的中心*</p><p><strong>API server</strong>(kube-apiserver): The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.</p><ul><li>作为Control plane的前端，是认证、授权、访问控制、API注册和发现等机制的统一入口，其中API为restful风格，同时交给etcd存储。</li></ul><p><strong><a href="https://etcd.io/docs/" target="_blank" rel="noopener">etcd</a></strong>: Consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data.</p><ul><li>一致且高度可用的键值存储，用作 Kubernetes 的所有集群数据的后台数据库。</li></ul><p><strong>scheduler</strong>(kube-scheduler): Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.</p><ol><li><p>负责节点(Node)的调度与监控，职责为监控新创建的、未指定运行Node的 Pods，并选择Node来让 Pod 运行。</p></li><li><p>调度决策考虑的因素包括单个 Pod 及 Pods 集合的资源需求、软硬件及策略约束、 亲和性及反亲和性规范、数据位置、工作负载间的干扰及最后时限。</p></li></ol><p><strong>controller-manager</strong></p><ol><li>kube-controller-manager: Control plane component that runs controller processes.<br>Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.</li></ol><ul><li>用于处理集群中常规后台任务，一个资源对应一个控制器，这些控制器包括：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">节点控制器（Node Controller）：</span><br><span class="line">    负责在节点出现故障时进行通知和响应</span><br><span class="line">任务控制器（Job Controller）：</span><br><span class="line">    监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成</span><br><span class="line">端点分片控制器（EndpointSlice controller）：</span><br><span class="line">    填充端点分片（EndpointSlice）对象（以提供 Service 和 Pod 之间的链接）。</span><br><span class="line">服务账号控制器（ServiceAccount controller）：</span><br><span class="line">    为新的命名空间创建默认的服务账号（ServiceAccount）。</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li>cloud-controller-manager:<br>云控制器管理器允许用户将集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与用户的集群交互的组件分离开来。</li></ol><ul><li>与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的控制回路组合到同一个可执行文件中，以同一进程的方式运行。 用户可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">节点控制器（Node Controller）：</span><br><span class="line">    用于在节点终止响应后检查云提供商以确定节点是否已被删除</span><br><span class="line">路由控制器（Route Controller）：</span><br><span class="line">    用于在底层云基础架构中设置路由</span><br><span class="line">服务控制器（Service Controller）：</span><br><span class="line">    用于创建、更新和删除云提供商负载均衡器</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-Node"><a href="#2-Node" class="headerlink" title="2.Node"></a>2.Node</h3><p><strong>kubelet</strong>: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.</p><ul><li>管理本机容器一个集群中每个节点上运行的代理(agent, not proxy)，它保证容器都运行在Pod中负责维护容器的生命周期，同时也负责Volume(CSI，容器存储接口) 和 网络(CNI，容器网络接口)的管理</li></ul><p><strong>kube-proxy</strong>: kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.</p><ul><li>提供网络代理，负载均衡等操作</li></ul><p><strong>Container Runtime</strong>: Docker(Mainly)</p><ul><li>Docker、containerd、cri-o、rktlet以及任何实现Kubernetes CRI (容器运行环境接口) 的软件。<h2 id="II-Kubernetes-WorkLoads"><a href="#II-Kubernetes-WorkLoads" class="headerlink" title="II. Kubernetes WorkLoads"></a>II. Kubernetes WorkLoads</h2><h3 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h3>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.</li><li>是k8s中最小的单元</li><li>一组容器的集合</li><li>一个Pod中的所有容器共享同一网络</li><li>生命周期是短暂的（服务器重启后，就找不到了）</li></ul><p>其中<strong>kubectl</strong>是Kubernetes集群的命令行接口, 假设一个demopod.yaml:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span> <span class="comment">#kubeapi的版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span> <span class="comment">#Pod/Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">PodName</span> </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">Container</span> <span class="string">Name</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">waynamigo/java:runtime</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">    - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure><p>那么由该yaml启动pod的命令格式为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl [<span class="built_in">command</span>] [TYPE] [NAME] [flags]</span><br><span class="line">kubectl apply -f demopod.yaml</span><br></pre></td></tr></table></figure><p><strong>组织形式</strong></p><ul><li><p>1Pod-1Container. 可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。</p></li><li><p>1Pod-NContainer. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources.<br>一个Pod可封装(encapsulate)由多个紧密耦合(coupled)且需要共享资源的容器组成的应用程序。Pod 将这些容器和存储资源打包为一个可管理的实体。</p></li></ul><p><strong>资源管理方式</strong></p><p>Pod 被设计成支持形成内聚服务单元的多个协作过程，提供两种共享资源：<strong>网络，存储(Volume)</strong>，使成员容器间能够进行数据共享和通信。</p><p><strong>更新与替换 - Update &amp; Replacement</strong></p><ol><li><p>当某Workload的 Pod Template被改变时，Controller会基于更新的模板创建新的 Pod对象，而不是对现有 Pod执行更新或者修补操作。</p></li><li><p>如果对Pod的某些字段执行 patch 和 replace 等更新操作，则有一些限制：</p></li></ol><ul><li><p>Pod 的绝大多数元数据都是不可变的。例如，用户不可以改变其 namespace、name、 uid 或者 creationTimestamp 字段；generation 字段是比较特别的， 如果更新该字段，只能增加字段取值而不能减少。</p></li><li><p>如果 metadata.deletionTimestamp 已经被设置，则不可以向 metadata.finalizers 列表中添加新的条目。</p></li><li><p>Pod 更新不可以改变除 spec.containers.image、spec.initContainers.image、 spec.activeDeadlineSeconds 或 spec.tolerations 之外的字段。 对于 spec.tolerations，用户只被允许添加新的条目。</p></li><li><p>在更新 spec.activeDeadlineSeconds 字段时，以下两种更新操作是被允许的：如果该字段尚未设置，可以将其设置为一个正数；<br>如果该字段已经设置为一个正数，可以将其设置为一个更小的、非负的整数。</p></li></ul><p><strong>其他</strong></p><p>*<a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/" target="_blank" rel="noopener">生命周期</a>*：</p><ul><li>Pending: 起始状态，</li><li>Running: 至少有一个主要容器正常启动，进入Running</li><li>Succeeded/Failed: 取决于 Pod 中是否有容器以失败状态结束</li><li>Unknown: 因为某些原因, 无法取得 Pod 状态。这种情况通常是因为与 Pod 所在主机通信失败。</li></ul><p>*<a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/" target="_blank" rel="noopener">Probe</a>*：容器探针是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 可以执行三种动作：</p><ul><li>ExecAction（借助容器运行时执行）</li><li>TCPSocketAction（由 kubelet 直接检测）</li><li>HTTPGetAction（由 kubelet 直接检测）</li></ul><p><em>特权模式</em>：在 Linux 中，Pod 中的任何容器都可以使用容器规约中的 安全性上下文中的 privileged 参数启用特权模式。 </p><p><em>Static Pod</em>：不通过API-server进行管理，直接由特定节点上的 kubelet 守护进程管理，通过 kubelet 直接监控每个 Pod，并在其失效时重启。并且不能引用其他的API对象。</p><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>Kubernetes 支持很多类型的卷：</p><ul><li>Volume声明在Pod容器中可访问的文件目录</li><li>一个Pod 可以同时使用任意数目的卷类型。 </li><li>可以被挂载到Pod中一个或多个容器指定路径下-</li><li>Pod 配置中的每个容器必须独立指定各个卷的挂载位置</li><li>支持多种后端存储抽象【本地存储、分布式存储、云存储】</li><li>临时卷类型的生命周期与 Pod 相同</li><li>对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。</li></ul><p><strong>持久卷</strong> Persistent Volume：是集群中的一块存储，可以由管理员事先制备， 或者使用存储类（Storage Class）来动态制备。</p><p><strong>投射卷</strong> Projected Volume：一个投射卷可以将若干现有的卷源映射到同一个目录之上</p><p><strong>临时卷</strong> Ephemeral Volume：有些应用程序需要额外的存储，但并不关心数据在重启后是否仍然可用。 </p><ul><li><p>缓存服务经常受限于内存大小，而且可以将不常用的数据转移到比内存慢的存储中，对总体性能的影响并不大。</p></li><li><p>另有些应用程序需要以文件形式注入的只读数据，比如配置数据或密钥。</p><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3></li><li><p>将当前状态（Current State）更新为期望状态（Desired State）</p></li><li><p>确保预期的pod副本数量【ReplicaSet】</p></li><li><p>无状态应用部署【Deployment】，无状态就是指，不需要依赖于网络或者ip</p></li><li><p>有状态应用部署【StatefulSet】，有状态即需要满足特定的初始条件进行部署</p></li><li><p>确保所有的node运行同一个pod 【DaemonSet】</p></li><li><p>一次性任务和定时任务【Job和CronJob】</p></li></ul><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>Deployment 为 Pod 和其副本(ReplicaSet)提供声明式的更新。</p><ul><li>用户负责描述 Deployment 中的 目标状态，（Controller） 以可控的速度更改实际运行状态（Current State)， 使其变为期望状态(Desired State)。</li></ul><p><strong>创建Deployment</strong></p><ul><li>如下demodeployment.yaml，用户可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 分配其资源。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">depName</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">nginx:1.14.2</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure></li></ul><p>该 Deployment 创建一个 ReplicaSet，包含3个Pod 副本。<br>同样通过kubectl运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f demodeployment.yaml</span><br><span class="line"><span class="comment"># 创建后通过get deployments进行获取运行时信息如下</span></span><br><span class="line">$ kubectl get deployments</span><br><span class="line">NAME      READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">depName   0/3     0            0           1s</span><br><span class="line"><span class="comment"># 查看 Deployment 上线状态</span></span><br><span class="line">$ kubectl rollout status deployment/depName</span><br><span class="line"><span class="comment"># 查看 Deployment 创建的 ReplicaSet（rs）</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                 DESIRED   CURRENT   READY   AGE</span><br><span class="line">depName-75675f5897   3         3         3       18s</span><br><span class="line"><span class="comment"># 查看每个 Pod 自动生成的标签</span></span><br><span class="line">$ kubectl get pods --show-labels</span><br></pre></td></tr></table></figure><ul><li>NAME 列出了名字空间中 Deployment 的名称。</li><li>READY 显示应用程序的可用的“副本”数。显示的模式是“就绪个数/期望个数”。</li><li>UP-TO-DATE 显示为了达到期望状态已经更新的副本数。</li><li>AVAILABLE 显示应用可供用户使用的副本数。</li><li>AGE 显示应用程序运行的时间。</li></ul><p><strong>更新/回滚/缩放/暂停 Deployments</strong></p><p>先来更新上述Pod的container，以使用 nginx:1.16.1 镜像，而不是 nginx:1.14.2 镜像。命令格式如 <figure class="highlight plain"><figcaption><span>set image deployment/metadata.name.depName spec.template.spec.containers[0]image</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">```bash</span><br><span class="line"># 创建</span><br><span class="line">$ kubectl set image deployment/depName nginx=nginx:1.16.1</span><br><span class="line">output: deployment.apps/depName edited</span><br><span class="line"># 查看deployment的details</span><br><span class="line">$ kubectl describe deployments</span><br><span class="line"># 查看上线状态</span><br><span class="line">$ kubectl rollout status deployment/depName</span><br><span class="line">output: </span><br><span class="line">1. Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">2. deployment &quot;depName&quot; successfully rolled out</span><br><span class="line"># 查看更新后的pod states</span><br><span class="line">$ kubectl get pods</span><br></pre></td></tr></table></figure></p><p>回滚操作和git的回滚操作类似</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查deployment修改历史</span></span><br><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment/depName</span><br><span class="line">output:</span><br><span class="line">deployments <span class="string">"depname"</span></span><br><span class="line">REVISION    CHANGE-CAUSE</span><br><span class="line">1           kubectl apply --filename=demodeployment.yaml</span><br><span class="line">2           kubectl <span class="built_in">set</span> image deployment/depName nginx=nginx:1.16.1</span><br><span class="line">3           kubectl <span class="built_in">set</span> image deployment/depName nginx=nginx:1.161</span><br><span class="line"><span class="comment"># 可以通过以下方式设置 CHANGE-CAUSE 消息：</span></span><br><span class="line">$ kubectl annotate deployment/depname kubernetes.io/change-cause=<span class="string">"image updated to 1.16.1"</span></span><br><span class="line"><span class="comment"># 查看某个revision的详细信息 可以通过--revisionc参数指定版本：</span></span><br><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment/depName --revision=2</span><br><span class="line"><span class="comment"># 回滚到上一个版本</span></span><br><span class="line">$ kubectl rollout undo deployment/depName</span><br><span class="line"><span class="comment"># 回滚到指定版本 --to-revision=2</span></span><br><span class="line">$ kubectl rollout undo deployment/depName --to-revision=2</span><br><span class="line">output:</span><br><span class="line">deployment.apps/depName rolled back</span><br><span class="line">$ kubectl describe deployment depName</span><br></pre></td></tr></table></figure><p>缩放deployment，即更新replicas，让rs的副本增加或减少</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定量缩放，比如replicas由3变5个</span></span><br><span class="line">$ kubectl scale deployment/depName --replicas=5</span><br><span class="line"><span class="comment"># 开启Pod的水平自动缩放后，根据cpu利用率设置pod运行个数的上下限</span></span><br><span class="line">$ kubectl autoscale deployment/depName --min=10 --max=15 --cpu-percent=80</span><br><span class="line"><span class="comment"># 限定可共享的资源</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> resources deployment/depName -c=nginx --limits=cpu=200m,memory=512Mi</span><br></pre></td></tr></table></figure><p>同时具有比例缩放特性 Proportional scaling </p><p>//TODO::命令设置</p><p>暂停deployment</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout pause deployment/depName</span><br></pre></td></tr></table></figure><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><ol><li>Service定义了一组pod的访问规则(An abstract way to expose an application running on a set of Pods as a network service.)</li><li>Pod的负载均衡，提供一个或多个Pod的稳定访问地址</li><li>支持多种方式【ClusterIP、NodePort、LoadBalancer】</li></ol><p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a <a href="https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/" target="_blank" rel="noopener">selector</a>.<br><strong>服务发现</strong></p><ol><li>Service in Kubernetes is a REST object, similar to a Pod. Like all of the REST objects, you can POST a Service definition to the API server to create a new instance. </li><li>用户想要在应用程序中使用 Kubernetes API 进行服务发现，则可以查询 API 服务器用于匹配 EndpointSlices：只要服务中的 Pod 集合发生更改，Kubernetes 就会为服务更新EndpointSlices。</li></ol><p>定义一个demoservice.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demoservice</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">MyApp</span> <span class="comment"># 使用该sector辨别pods组</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p><strong>Ingress</strong></p><ol><li>首先，Ingress是公开从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。</li><li>Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。</li><li>Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。</li><li>An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer.</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">minimal-ingress</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ingressClassName:</span> <span class="string">nginx-example</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/testpath</span></span><br><span class="line"><span class="attr">        pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          service:</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">            port:</span></span><br><span class="line"><span class="attr">              number:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇为OverView，内容包括kubectl的基础操作，整理的知识框架基于kubernetes官方文档&lt;a href=&quot;https://kubernetes.io/docs/tutorials/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v1.26&lt;/a&gt;， 元旦期间系统整理一下。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="http://waynamigo.github.io/categories/Kubernetes/"/>
    
    
      <category term="CloudComputing" scheme="http://waynamigo.github.io/tags/CloudComputing/"/>
    
      <category term="Kubernetes" scheme="http://waynamigo.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>业务数据需求及数据处理概要-水上机器视觉场景</title>
    <link href="http://waynamigo.github.io/2022/10/01/2022-10-01-%E6%B0%B4%E4%B8%8A%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89-%E6%A1%A5%E6%B4%9E%E5%9C%BA%E6%99%AF%E4%B8%9A%E5%8A%A1%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1/"/>
    <id>http://waynamigo.github.io/2022/10/01/2022-10-01-水上机器视觉-桥洞场景业务功能模块设计/</id>
    <published>2022-09-30T16:00:00.000Z</published>
    <updated>2023-01-04T06:40:06.334Z</updated>
    
    <content type="html"><![CDATA[<p>水上机器视觉场景业务分析</p><a id="more"></a><h1 id="web摄像头即时视频流"><a href="#web摄像头即时视频流" class="headerlink" title="web摄像头即时视频流"></a>web摄像头即时视频流</h1><p>初始数据格式：船载web摄像头获取的视频流，基于本地流媒体服务传输到系统（rtmp/rtsp）</p><p>处理方式：ffmpeg解码视频流到rgb图像帧，经测试基于nginx的rtmp流媒体在本地延迟在80ms左右（局域网场景，摄像头参数为1080p双摄，nginx流媒体部署于树莓派3b+）</p><p>处理后数据格式：ImageFrame - 以 <strong>{测试样例_时间戳}.jpg</strong> 命名</p><h1 id="点云数据"><a href="#点云数据" class="headerlink" title="点云数据"></a>点云数据</h1><p>「二维平面（综合判定的）点云的点以圆输出，给出圆心坐标、圆半径」 <em>from 水上机器视觉技术讨论_会议纪要.doc</em></p><p>初始数据格式：点云中单个点以圆心坐标、圆半径的形式给出？<br>            能否理解为：将点云做一次拟合圆之后，给出船体所在水平面的点云</p><p>处理方式：能否直接给出拟合圆以前的世界坐标系/雷达坐标系下的点云PointXYZ，和雷达轨迹（四元数或欧拉角描述）</p><p>处理后数据格式：<br>            每一图像帧时刻的PCL处理的点云数据，以PointXYZ描述 { pointid, x, y, z}<br>            每一图像帧时刻的雷达轨迹，以四元数或欧拉角描述    { w, x, y, z}/{ x, y, z, rx, ry, rz}</p><h1 id="视觉雷达联合标定"><a href="#视觉雷达联合标定" class="headerlink" title="视觉雷达联合标定"></a>视觉雷达联合标定</h1><p>初始数据格式：光学相机：相机内参，畸变函数；相机外参<br>            雷达： 雷达外参数。<br>处理方式：<br>将毫米波雷达返回的目标点投影到图像上，围绕该点并结合先验知识，生成一个矩形的感兴趣区域，然后我们只对该区域内进行识别（水岸分割模型或目标检测模型）。优点是<br>可以迅速地排除大量不会有目标的区域，极大地提高识别速度。<br>建立精确的毫米波雷达坐标系、三维世界坐标系、摄像机坐标系、图像坐标系和像素坐标系之间的坐标转换关系，是实现毫米波和视觉融合的关键。<br>毫米波雷达与视觉传感器在空间的融合就是将不同传感器坐标系的测量值转换到同一个坐标系中。由于ADAS前向视觉系统以视觉为主，因此只需将毫米雷达坐标系下的测量点<br>通过坐标系转换到摄像机对应的像素坐标系下即可实现两者空间同步。<br>联合标定的目的：将毫米波检测的目标转换到图像上。</p><p>联合标定方式（原理）：</p><ul><li>毫米波坐标系下的坐标转换到以相机为中心的世界坐标系中</li><li>将世界坐标系的坐标转换到相机坐标系</li><li>将相机坐标系的坐标转换到图像坐标系</li></ul><p>毫米波可以得到目标在图像中的x,y坐标信息（文档中提到的拟合平面，该xy是在毫米波坐标系下的数值，2D），由于没有目标的z坐标信息，<br>可以由（x,y,1）将毫米波坐标系转换到相机世界坐标系下，</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;水上机器视觉场景业务分析&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://waynamigo.github.io/categories/Notes/"/>
    
    
      <category term="PointCloud" scheme="http://waynamigo.github.io/tags/PointCloud/"/>
    
  </entry>
  
  <entry>
    <title>Paper Routing for Visual Grounding</title>
    <link href="http://waynamigo.github.io/2022/10/01/2022-10-01-VGApproachs/"/>
    <id>http://waynamigo.github.io/2022/10/01/2022-10-01-VGApproachs/</id>
    <published>2022-09-30T16:00:00.000Z</published>
    <updated>2023-01-04T05:32:52.312Z</updated>
    
    <content type="html"><![CDATA[<p>Visual Grounding<br>Referring Expressions<br>Phrase Grounding</p><a id="more"></a><h2 id="Reprensentation-Approach"><a href="#Reprensentation-Approach" class="headerlink" title="Reprensentation Approach"></a>Reprensentation Approach</h2><p>几个在VG任务中的主流视觉backbone</p><ul><li><strong>rpn</strong></li><li><strong>maskrcnn</strong></li><li><strong>retinanet(fpn)</strong></li><li><strong>Vit</strong></li><li><strong>DETR</strong></li></ul><p>文本表示的编码方式/编码器模型</p><ul><li><strong>word2vec</strong> <a href="2D_VisualGrounding/word2vec.pdf">[File]</a></li><li><strong>bert</strong></li><li></li></ul><h2 id="VG-paper-routing"><a href="#VG-paper-routing" class="headerlink" title="VG paper routing"></a>VG paper routing</h2><ol><li><p>Karpathy, Andrej, Armand Joulin, and Li F. Fei-Fei. <strong>Deep fragment embeddings for bidirectional image sentence mapping.</strong> Advances in neural information processing systems. 2014. <a href="http://papers.nips.cc/paper/5281-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping.pdf" target="_blank" rel="noopener">[Paper]</a></p><h3 id="RNN类方法"><a href="#RNN类方法" class="headerlink" title="RNN类方法"></a>RNN类方法</h3></li><li><p>Karpathy, Andrej, and Li Fei-Fei. <strong>Deep visual-semantic alignments for generating image descriptions.</strong> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. <em>Method name: Neural Talk</em>. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/karpathy/neuraltalk" target="_blank" rel="noopener">[Code]</a> <a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="noopener">[Torch Code]</a> <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/" target="_blank" rel="noopener">[Website]</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RPN作为视觉backbone+BiRNN编码文本，前19个region和karpathy分割的snippets（phrase）映射到同一长度vector后进行相似度计算S，max(0,S)以衡量整个图片与句子的相似程度。</span><br><span class="line"></span><br><span class="line">* 整体是用的retrieval的baseline，类似于SCAN等retrival任务的特征处理方式</span><br></pre></td></tr></table></figure></li><li><p>Hu, Ronghang, et al. <strong>Natural language object retrieval.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. </p></li></ol><p><strong>Method name</strong>: Spatial Context Recurrent<br>ConvNet (SCRC)* <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hu_Natural_Language_Object_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ronghanghu/natural-language-object-retrieval" target="_blank" rel="noopener">[Code]</a> <a href="http://ronghanghu.com/text_obj_retrieval/" target="_blank" rel="noopener">[Website]</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本提首先进入一个embedding层</span><br><span class="line">CNN同样提取global contextual feature和 local feature，</span><br><span class="line">LSTM 获取local 和 global信息（两个单元），local处理[x box ,x spatial]</span><br></pre></td></tr></table></figure><ol><li><p>Mao, Junhua, et al. <strong>Generation and comprehension of unambiguous object descriptions.</strong> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. <a href="https://arxiv.org/pdf/1511.02283.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/mjhucla/Google_Refexp_toolbox" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Wang, Liwei, Yin Li, and Svetlana Lazebnik. <strong>Learning deep structure-preserving image-text embeddings.</strong> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. <a href="http://slazebni.cs.illinois.edu/publications/cvpr16_structure.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/lwwang/Two_branch_network" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yu, Licheng, et al. <strong>Modeling context in referring expressions.</strong> European Conference on Computer Vision. Springer, Cham, 2016. <a href="https://arxiv.org/pdf/1608.00272.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/lichengunc/refer" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. <strong>Modeling context between objects for referring expression understanding.</strong> European Conference on Computer Vision. Springer, Cham, 2016.<a href="https://arxiv.org/pdf/1608.00525.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/varun-nagaraja/referring-expressions" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Rohrbach, Anna, et al. <strong>Grounding of textual phrases in images by reconstruction.</strong> European Conference on Computer Vision. Springer, Cham, 2016. <em>Method Name: GroundR</em> <a href="https://arxiv.org/pdf/1511.03745.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/kanchen-usc/GroundeR" target="_blank" rel="noopener">[Tensorflow Code]</a> <a href="https://github.com/ruotianluo/refexp-comprehension" target="_blank" rel="noopener">[Torch Code]</a></p></li><li><p>Wang, Mingzhe, et al. <strong>Structured matching for phrase localization.</strong> European Conference on Computer Vision. Springer, Cham, 2016. <em>Method name: Structured Matching</em> <a href="https://pdfs.semanticscholar.org/9216/2ec88ad974cc5082d9688c8bfee672ad59ad.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/princeton-vl/structured-matching" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Hu, Ronghang, Marcus Rohrbach, and Trevor Darrell. <strong>Segmentation from natural language expressions.</strong> European Conference on Computer Vision. Springer, Cham, 2016. <a href="https://arxiv.org/pdf/1603.06180.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ronghanghu/text_objseg" target="_blank" rel="noopener">[Code]</a> <a href="http://ronghanghu.com/text_objseg/" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Fukui, Akira et al. <strong>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.</strong> EMNLP (2016). <em>Method name: MCB</em> <a href="https://arxiv.org/pdf/1606.01847.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/akirafukui/vqa-mcb" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Endo, Ko, et al. <strong>An attention-based regression model for grounding textual phrases in images.</strong> Proc. IJCAI. 2017. <a href="https://www.ijcai.org/proceedings/2017/0558.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Chen, Kan, et al. <strong>MSRC: Multimodal spatial regression with semantic context for phrase grounding.</strong> International Journal of Multimedia Information Retrieval 7.1 (2018): 17-28. <a href="https://link.springer.com/article/10.1007/s13735-017-0139-6" target="_blank" rel="noopener">[Paper -Springer Link]</a></p></li><li><p>Wu, Fan et al. <strong>An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning.</strong> CoRR abs/1703.07579 (2017): n. pag. <a href="https://arxiv.org/pdf/1703.07579.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/jxwufan/NLOR_A3C" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yu, Licheng, et al. <strong>A joint speakerlistener-reinforcer model for referring expressions.</strong> Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_A_Joint_Speaker-Listener-Reinforcer_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/lichengunc/speaker_listener_reinforcer" target="_blank" rel="noopener">[Code]</a><a href="https://vision.cs.unc.edu/refer/" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Hu, Ronghang, et al. <strong>Modeling relationships in referential expressions with compositional modular networks.</strong> Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Modeling_Relationships_in_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ronghanghu/cmn" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Luo, Ruotian, and Gregory Shakhnarovich. <strong>Comprehension-guided referring expressions.</strong> Computer Vision and Pattern Recognition (CVPR). Vol. 2. 2017. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Luo_Comprehension-Guided_Referring_Expressions_CVPR_2017_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ruotianluo/refexp-comprehension" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Jingyu, Liang Wang, and Ming-Hsuan Yang. <strong>Referring expression generation and comprehension via attributes.</strong> Proceedings of CVPR. 2017. <a href="http://faculty.ucmerced.edu/mhyang/papers/iccv2017_referring_expression.pdf" target="_blank" rel="noopener">[Paper]</a> </p></li><li><p>Xiao, Fanyi, Leonid Sigal, and Yong Jae Lee. <strong>Weakly-supervised visual grounding of phrases with linguistic structures.</strong> arXiv preprint arXiv:1705.01371 (2017). <a href="https://arxiv.org/pdf/1705.01371.pdf" target="_blank" rel="noopener">[Paper]</a> </p></li><li><p>Plummer, Bryan A., et al. <strong>Phrase localization and visual relationship detection with comprehensive image-language cues.</strong> Proc. ICCV. 2017. <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Plummer_Phrase_Localization_and_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/BryanPlummer/pl-clc" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Chen, Kan, Rama Kovvuri, and Ram Nevatia. <strong>Query-guided regression network with context policy for phrase grounding.</strong> Proceedings of the IEEE International Conference on Computer Vision (ICCV). 2017. <em>Method name: QRC</em> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Query-Guided_Regression_Network_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/kanchen-usc/QRC-Net" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Chenxi, et al. <strong>Recurrent Multimodal Interaction for Referring Image Segmentation.</strong> ICCV. 2017. <a href="https://arxiv.org/pdf/1703.07939.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/chenxi116/TF-phrasecut-public" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Li, Jianan, et al. <strong>Deep attribute-preserving metric learning for natural language object retrieval.</strong> Proceedings of the 2017 ACM on Multimedia Conference. ACM, 2017. <a href="https://dl.acm.org/citation.cfm?id=3123439" target="_blank" rel="noopener">[Paper: ACM Link]</a></p></li><li><p>Li, Xiangyang, and Shuqiang Jiang. <strong>Bundled Object Context for Referring Expressions.</strong> IEEE Transactions on Multimedia (2018). <a href="https://ieeexplore.ieee.org/document/8307406" target="_blank" rel="noopener">[Paper ieee link]</a> </p></li><li><p>Yu, Zhou, et al. <strong>Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding.</strong> arXiv preprint arXiv:1805.03508 (2018). <a href="https://www.ijcai.org/proceedings/2018/0155.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/XiangChenchao/DDPN" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yu, Licheng, et al. <strong>Mattnet: Modular attention network for referring expression comprehension.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/lichengunc/MAttNet" target="_blank" rel="noopener">[Code]</a> <a href="http://vision2.cs.unc.edu/refer/comprehension" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Deng, Chaorui, et al. <strong>Visual Grounding via Accumulated Attention.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Li, Ruiyu, et al. <strong>Referring image segmentation via recurrent refinement networks.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Referring_Image_Segmentation_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/liruiyu/referseg_rrn" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Zhang, Yundong, Juan Carlos Niebles, and Alvaro Soto. <strong>Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining.</strong> arXiv preprint arXiv:1808.00265 (2018). <a href="https://arxiv.org/pdf/1808.00265.pdf" target="_blank" rel="noopener">[Paper]</a> </p></li><li><p>Chen, Kan, Jiyang Gao, and Ram Nevatia. <strong>Knowledge aided consistency for weakly supervised phrase grounding.</strong> arXiv preprint arXiv:1803.03879 (2018). <a href="https://arxiv.org/abs/1803.03879" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/kanchen-usc/KAC-Net" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Zhang, Hanwang, Yulei Niu, and Shih-Fu Chang. <strong>Grounding referring expressions in images by variational context.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/yuleiniu/vc/" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Cirik, Volkan, Taylor Berg-Kirkpatrick, and Louis-Philippe Morency. <strong>Using syntax to ground referring expressions in natural images.</strong> arXiv preprint arXiv:1805.10547 (2018).<a href="https://arxiv.org/pdf/1805.10547.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/volkancirik/groundnet" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Margffoy-Tuay, Edgar, et al. <strong>Dynamic multimodal instance segmentation guided by natural language queries.</strong> Proceedings of the European Conference on Computer Vision (ECCV). 2018. <a href="https://arxiv.org/pdf/1807.02257.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/BCV-Uniandes/DMS" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Shi, Hengcan, et al. <strong>Key-word-aware network for referring expression image segmentation.</strong> Proceedings of the European Conference on Computer Vision (ECCV). 2018.<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hengcan_Shi_Key-Word-Aware_Network_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/shihengcan/key-word-aware-network-pycaffe" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Plummer, Bryan A., et al. <strong>Conditional image-text embedding networks.</strong> Proceedings of the European Conference on Computer Vision (ECCV). 2018. <a href="https://arxiv.org/pdf/1711.08389.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/BryanPlummer/cite" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Akbari, Hassan, et al. <strong>Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding.</strong> arXiv preprint arXiv:1811.11683 (2018). <a href="https://arxiv.org/pdf/1811.11683v1.pdf" target="_blank" rel="noopener">[Paper]</a> </p></li><li><p>Kovvuri, Rama, and Ram Nevatia. <strong>PIRC Net: Using Proposal Indexing, Relationships and Context for Phrase Grounding.</strong> arXiv preprint arXiv:1812.03213 (2018). <a href="https://arxiv.org/pdf/1812.03213v1.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Chen, Xinpeng, et al. <strong>Real-Time Referring Expression Comprehension by Single-Stage Grounding Network.</strong> arXiv preprint arXiv:1812.03426 (2018). <a href="https://arxiv.org/pdf/1812.03426v1.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Wang, Peng, et al. <strong>Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks.</strong> arXiv preprint arXiv:1812.04794 (2018). <a href="https://arxiv.org/pdf/1812.04794.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Liu, Daqing, et al. <strong>Learning to Assemble Neural Module Tree Networks for Visual Grounding.</strong> Proceedings of the IEEE International Conference on Computer Vision (ICCV). 2019. <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/daqingliu/NMTree" target="_blank" rel="noopener">[Code]</a></p></li><li><p><strong>RETRACTED (see <a href="https://github.com/TheShadow29/awesome-grounding/pull/2" target="_blank" rel="noopener">#2</a>)</strong>:  Deng, Chaorui, et al. <strong>You Only Look &amp; Listen Once: Towards Fast and Accurate Visual Grounding.</strong> arXiv preprint arXiv:1902.04213 (2019). <a href="https://arxiv.org/pdf/1902.04213.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Hong, Richang, et al. <strong>Learning to Compose and Reason with Language Tree Structures for Visual Grounding.</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI). 2019. <a href="https://arxiv.org/pdf/1906.01784.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Liu, Xihui, et al. <strong>Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. <a href="https://arxiv.org/pdf/1903.00839.pdf" target="_blank" rel="noopener">[Paper]</a> </p></li><li><p>Dogan, Pelin, Leonid Sigal, and Markus Gross. <strong>Neural Sequential Phrase Grounding (SeqGROUND).</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2019. <a href="https://arxiv.org/pdf/1903.07669.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Datta, Samyak, et al. <strong>Align2ground: Weakly supervised phrase grounding guided by image-caption alignment.</strong> arXiv preprint arXiv:1903.11649 (2019). (ICCV 2019) <a href="https://arxiv.org/pdf/1903.11649.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Fang, Zhiyuan, et al. <strong>Modularized textual grounding for counterfactual resilience.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2019. <a href="https://arxiv.org/pdf/1904.03589.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Ye, Linwei, et al. <strong>Cross-Modal Self-Attention Network for Referring Image Segmentation.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2019. <a href="https://arxiv.org/pdf/1904.04745.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Yang, Sibei, Guanbin Li, and Yizhou Yu. <strong>Cross-Modal Relationship Inference for Grounding Referring Expressions.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2019. <a href="https://arxiv.org/pdf/1906.04464.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Yang, Sibei, Guanbin Li, and Yizhou Yu. <strong>Dynamic Graph Attention for Referring Expression Comprehension.</strong> arXiv preprint arXiv:1909.08164 (2019). (ICCV 2019) <a href="https://arxiv.org/pdf/1909.08164.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/sibeiyang/sgmn/tree/master/lib/dga_models" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Wang, Josiah, and Lucia Specia. <strong>Phrase Localization Without Paired Training Examples.</strong> arXiv preprint arXiv:1908.07553 (2019). (ICCV 2019) <a href="https://arxiv.org/abs/1908.07553" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/josiahwang/phraseloceval" target="_blank" rel="noopener">[Code]</a> </p></li><li><p>Yang, Zhengyuan, et al. <strong>A Fast and Accurate One-Stage Approach to Visual Grounding.</strong> arXiv preprint arXiv:1908.06354 (2019). (ICCV 2019) <a href="https://arxiv.org/pdf/1908.06354.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/zyang-ur/onestage_grounding" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Sadhu, Arka, Kan Chen, and Ram Nevatia. <strong>Zero-Shot Grounding of Objects from Natural Language Queries.</strong> arXiv preprint arXiv:1908.07129 (2019).(ICCV 2019) <a href="https://arxiv.org/abs/1908.07129" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/TheShadow29/zsgnet-pytorch" target="_blank" rel="noopener">[Code]</a></p></li></ol><p><em>Disclaimer: I am an author of the paper</em></p><ol><li><p>Liu, Xuejing, et al. <strong>Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding.</strong> arXiv preprint arXiv:1908.10568 (2019). (ICCV 2019) <a href="https://arxiv.org/pdf/1908.10568.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/GingL/ARN" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Chen, Yi-Wen, et al. <strong>Referring Expression Object Segmentation with Caption-Aware Consistency.</strong> arXiv preprint arXiv:1910.04748 (2019). (BMVC 2019) <a href="https://arxiv.org/abs/1910.04748" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/wenz116/lang2seg" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Jiacheng, and Julia Hockenmaier. <strong>Phrase Grounding by Soft-Label Chain Conditional Random Field.</strong> arXiv preprint arXiv:1909.00301 (2019) (EMNLP 2019). <a href="https://arxiv.org/pdf/1909.00301.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/liujch1998/SoftLabelCCRF" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Yongfei, Wan Bo, Zhu Xiaodan and He Xuming. <strong>Learning Cross-modal Context Graph for Visual Grounding.</strong> arXiv preprint arXiv: (2019) (AAAI-2020). <a href="https://arxiv.org/pdf/1911.09042.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/youngfly11/LCMCG-PyTorch" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yu, Tianyu, et al. <strong>Cross-Modal Omni Interaction Modeling for Phrase Grounding.</strong> Proceedings of the 28th ACM International Conference on Multimedia. ACM 2020. <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413846" target="_blank" rel="noopener">[Paper: ACM Link]</a> <a href="https://github.com/yiranyyu/Phrase-Grounding" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Qiu, Heqian, et al. <strong>Language-Aware Fine-Grained Object Representation for Referring Expression Comprehension.</strong> Proceedings of the 28th ACM International Conference on Multimedia. ACM 2020. <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413850" target="_blank" rel="noopener">[Paper: ACM Link]</a></p></li><li><p>Wang, Qinxin, et al. <strong>MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding.</strong> arXiv preprint arXiv:2010.05379 (2020). <a href="https://arxiv.org/pdf/2010.05379" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/qinzzz/Multimodal-Alignment-Framework" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liao, Yue, et al. <strong>A real-time cross-modality correlation filtering method for referring expression comprehension.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression_Comprehension_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Hu, Zhiwei, et al. <strong>Bi-directional relationship inferring network for referring image segmentation.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/fengguang94/CVPR2020-BRINet" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yang, Sibei, Guanbin Li, and Yizhou Yu. <strong>Graph-structured referring expression reasoning in the wild.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020. <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Graph-Structured_Referring_Expression_Reasoning_in_the_Wild_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/sibeiyang/sgmn" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Luo, Gen, et al. <strong>Multi-task collaborative network for joint referring expression comprehension and segmentation.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020. <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Multi-Task_Collaborative_Network_for_Joint_Referring_Expression_Comprehension_and_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/luogen1996/MCN" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Gupta, Tanmay, et al. <strong>Contrastive learning for weakly supervised phrase grounding.</strong> Proceedings of the European Conference on Computer Vision (ECCV). 2020. <a href="https://arxiv.org/pdf/2006.09920" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/BigRedT/info-ground" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Yang, Zhengyuan, et al. <strong>Improving one-stage visual grounding by recursive sub-query construction.</strong> Proceedings of the European Conference on Computer Vision (ECCV). 2020. <a href="https://arxiv.org/pdf/2008.01059" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/zyang-ur/ReSC" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Wang, Liwei, et al. <strong>Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. <a href="https://arxiv.org/pdf/2007.01951" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Sun, Mingjie, Jimin Xiao, and Eng Gee Lim. <strong>Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. <a href="https://arxiv.org/pdf/2007.01951" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/insomnia94/ISREG" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Haolin, et al. <strong>Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. <a href="https://arxiv.org/pdf/2103.07894" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/UncleMEDM/Refer-it-in-RGBD" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Liu, Yongfei, et al. <strong>Relation-aware Instance Refinement for Weakly Supervised Visual Grounding.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. <a href="https://arxiv.org/pdf/2103.12989" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Lin, Xiangru, Guanbin Li, and Yizhou Yu. <strong>Scene-Intuitive Agent for Remote Embodied Visual Grounding.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2021. <a href="https://arxiv.org/pdf/2103.12944" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Sun, Mingjie, et al. <strong>Discriminative triad matching and reconstruction for weakly referring expression grounding.</strong> IEEE transactions on pattern analysis and machine intelligence (TPAMI 2021). <a href="https://livrepository.liverpool.ac.uk/3116000/1/manuscript.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/insomnia94/DTWREG" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Mu, Zongshen, et al. <strong>Disentangled Motif-aware Graph Learning for Phrase Grounding.</strong> arXiv preprint arXiv:2104.06008 (AAAI 2021). <a href="https://www.aaai.org/AAAI21Papers/AAAI-2589.MuZ.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Chen, Long, et al. <strong>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding.</strong> arXiv preprint arXiv:2009.01449 (AAAI-2021). <a href="https://arxiv.org/pdf/2009.01449" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ChopinSharp/ref-nms" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Deng, Jiajun, et al. <strong>TransVG: End-to-End Visual Grounding with Transformers.</strong> arXiv preprint arXiv:2104.08541 (2021). <a href="https://arxiv.org/pdf/2104.08541" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/nku-shengzheliu/Pytorch-TransVG" target="_blank" rel="noopener">[Unofficial Code]</a></p></li><li><p>Du, Ye, et al. <strong>Visual Grounding with Transformers.</strong> arXiv preprint arXiv:2105.04281 (2021). <a href="https://arxiv.org/pdf/2105.04281" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Kamath, Aishwarya, et al. <strong>MDETR–Modulated Detection for End-to-End Multi-Modal Understanding.</strong> arXiv preprint arXiv:2104.12763 (2021). <a href="https://arxiv.org/pdf/2104.12763" target="_blank" rel="noopener">[Paper]</a></p></li></ol><h3 id="Natural-Language-Object-Retrieval-Images"><a href="#Natural-Language-Object-Retrieval-Images" class="headerlink" title="Natural Language Object Retrieval (Images)"></a>Natural Language Object Retrieval (Images)</h3><ol><li><p>Guadarrama, Sergio, et al. <strong>Open-vocabulary Object Retrieval.</strong> Robotics: science and systems. Vol. 2. No. 5. 2014. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.3000&rep=rep1&type=pdf" target="_blank" rel="noopener">[Paper]</a> <a href="http://openvoc.berkeleyvision.org/" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Hu, Ronghang, et al. <strong>Natural language object retrieval.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. <em>Method name: Spatial Context Recurrent ConvNet (SCRC)</em> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hu_Natural_Language_Object_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/ronghanghu/natural-language-object-retrieval" target="_blank" rel="noopener">[Code]</a> <a href="http://ronghanghu.com/text_obj_retrieval/" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Wu, Fan et al. <strong>An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning.</strong> CoRR abs/1703.07579 (2017): n. pag. <a href="https://arxiv.org/pdf/1703.07579.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/jxwufan/NLOR_A3C" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Li, Jianan, et al. <strong>Deep attribute-preserving metric learning for natural language object retrieval.</strong> Proceedings of the 2017 ACM on Multimedia Conference. ACM, 2017. <a href="https://dl.acm.org/citation.cfm?id=3123439" target="_blank" rel="noopener">[Paper: ACM Link]</a></p></li><li><p>Nguyen, Anh, et al. <strong>Object Captioning and Retrieval with Natural Language.</strong> arXiv preprint arXiv:1803.06152 (2018). <a href="https://arxiv.org/pdf/1803.06152.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://sites.google.com/site/objcaptioningretrieval/" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Plummer, Bryan A., et al. <strong>Open-vocabulary Phrase Detection.</strong> arXiv preprint arXiv:1811.07212 (2018). <a href="https://arxiv.org/pdf/1811.07212.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/VisionLearningGroup/phrase-rcnn" target="_blank" rel="noopener">[Code]</a></p></li></ol><h3 id="Grounding-Relations-Referring-Relations"><a href="#Grounding-Relations-Referring-Relations" class="headerlink" title="Grounding Relations / Referring Relations"></a>Grounding Relations / Referring Relations</h3><ol><li><p>Krishna, Ranjay, et al. <strong>Referring relationships.</strong> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. <a href="https://arxiv.org/pdf/1803.10362.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/StanfordVL/ReferringRelationships" target="_blank" rel="noopener">[Code]</a> <a href="https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/index.html" target="_blank" rel="noopener">[Website]</a></p></li><li><p>Raboh, Moshiko et al. <strong>Differentiable Scene Graphs.</strong> (2019). <a href="https://arxiv.org/pdf/1902.10200.pdf" target="_blank" rel="noopener">[Paper]</a></p></li><li><p>Conser, Erik, et al. <strong>Revisiting Visual Grounding.</strong> arXiv preprint arXiv:1904.02225 (2019).<br><a href="https://arxiv.org/pdf/1904.02225.pdf" target="_blank" rel="noopener">[Paper]</a></p><ul><li>Critique of Referring Relationship paper</li></ul></li></ol><h3 id="Grounded-Description-Image-WIP"><a href="#Grounded-Description-Image-WIP" class="headerlink" title="Grounded Description (Image) (WIP)"></a>Grounded Description (Image) (WIP)</h3><ol><li><p>Hendricks, Lisa Anne, et al. <strong>Generating visual explanations.</strong> European Conference on Computer Vision. Springer, Cham, 2016. <a href="https://arxiv.org/pdf/1603.08507.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/LisaAnne/ECCV2016/" target="_blank" rel="noopener">[Code]</a> <a href="https://github.com/salaniz/pytorch-gve-lrcn" target="_blank" rel="noopener">[Pytorch Code]</a></p></li><li><p>Jiang, Ming, et al. <strong>TIGEr: Text-to-Image Grounding for Image Caption Evaluation.</strong> arXiv preprint arXiv:1909.02050 (2019). (EMNLP 2019) <a href="https://arxiv.org/pdf/1909.02050.pdf" target="_blank" rel="noopener">[Paper]</a> <a href="https://github.com/SeleenaJM/CapEval" target="_blank" rel="noopener">[Code]</a></p></li><li><p>Lee, Jason, Kyunghyun Cho, and Douwe Kiela. <strong>Countering language drift via visual grounding.</strong> arXiv preprint arXiv:1909.04499 (2019). (EMNLP 2019) <a href="https://arxiv.org/pdf/1909.04499.pdf" target="_blank" rel="noopener">[Paper]</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Visual Grounding&lt;br&gt;Referring Expressions&lt;br&gt;Phrase Grounding&lt;/p&gt;
    
    </summary>
    
      <category term="VisualGrounding" scheme="http://waynamigo.github.io/categories/VisualGrounding/"/>
    
    
      <category term="Multimodal" scheme="http://waynamigo.github.io/tags/Multimodal/"/>
    
      <category term="Paper" scheme="http://waynamigo.github.io/tags/Paper/"/>
    
      <category term="VisualGrounding" scheme="http://waynamigo.github.io/tags/VisualGrounding/"/>
    
  </entry>
  
  <entry>
    <title>点云业务开发(ROS,Autoware)及嵌入式开发的一些备忘</title>
    <link href="http://waynamigo.github.io/2022/09/14/2022-09-14-%E7%82%B9%E4%BA%91ROS%E4%B8%9A%E5%8A%A1/"/>
    <id>http://waynamigo.github.io/2022/09/14/2022-09-14-点云ROS业务/</id>
    <published>2022-09-13T16:00:00.000Z</published>
    <updated>2023-01-04T06:41:18.513Z</updated>
    
    <content type="html"><![CDATA[<p>ROS, Autoware编译, rk3399开发板在实验室网络设置等的一些备忘</p><a id="more"></a><p>conda :gcc,make,gpgme,libarchive,zstd,<br>conda lib for system /lib</p><p>pacmansource code<br><a href="https://sources.archlinux.org/other/pacman/" target="_blank" rel="noopener">https://sources.archlinux.org/other/pacman/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PKG_CHECK_MODULES(LIBARCHIVE, [libarchive &gt;= 3.0.0], ,</span><br><span class="line">AC_MSG_ERROR([*** libarchive &gt;= 3.0.0 is needed to compile pacman!]))</span><br></pre></td></tr></table></figure><h2 id="rk3399"><a href="#rk3399" class="headerlink" title="rk3399"></a>rk3399</h2><ol><li>4g module disble (for now):rk_wifi_init module(driver) is not able to execute</li><li>setting for static ip</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ip       : using DHCP in this room of ifconfig (after login in at UPC)</span><br><span class="line">gateway  : ip addr/ ifconfig/ netstat -rn</span><br><span class="line">netmask  : ip addr to check network segment and calculate yourself</span><br><span class="line"></span><br><span class="line">modify: /etc/network/interfaces</span><br><span class="line"></span><br><span class="line">auto [your eth name]</span><br><span class="line">iface [you eth name] inet [static]</span><br><span class="line">address [your ip]</span><br><span class="line">netmask [your netmask]</span><br><span class="line">gateway [your gateway]</span><br><span class="line">broadcast [your broadcast]</span><br><span class="line"></span><br><span class="line">ep.</span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet static</span><br><span class="line">address 180.201.136.11</span><br><span class="line">netmask 255.255.192.0</span><br><span class="line">gateway 180.201.128.1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 down</span><br><span class="line">ifconfig eth0 up</span><br></pre></td></tr></table></figure><ol start="3"><li>networkauth<br>if the <strong>issue</strong> below occured:<br>“Failed to establish a new connection: [Errno -2] Name or service not known…”<br>detail:<br>“requests.exceptions.ConnectionError: HTTPSConnectionPool(host=’xx’, port=443): Max retries exceeded with url: /appapi/exchange/19/v1/prolist (Caused by NewConnectionError(‘&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fca889818d0&gt;: Failed to establish a new connection: [Errno -2] Name or service not known’,))”</li></ol><p><strong>solution</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. PING lan.upc.edu.cn -&gt; (121.251.251.207) </span><br><span class="line">2. lan.upc.edu.cn &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>run networkAuth script</p><ol start="4"><li>cross-compile :: <a href="https://github.com/RangiLyu/nanodet" target="_blank" rel="noopener">https://github.com/RangiLyu/nanodet</a><br><a href="https://developer.arm.com/downloads/-/gnu-a" target="_blank" rel="noopener">gcc compiler toolchain</a><br><a href="https://developer.arm.com/-/media/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz?rev=1cb9c51b94f54940bdcccd791451cec3&hash=B380A59EA3DC5FDC0448CA6472BF6B512706F8EC" target="_blank" rel="noopener">for x86(host)-aarch64</a></li></ol><p><strong>compiler env</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. untar the toolchain package</span><br><span class="line">2. export PATH="&lt;toolchain-compiler-bin-path&gt;:$&#123;PATH&#125;"</span><br><span class="line">3. apt install g++-arm-linux-gnueabi g++-arm-linux-gnueabihf g++-aarch64-linux-gnu</span><br><span class="line"> execute on -- aarch64-none-linux-gnu-gcc</span><br></pre></td></tr></table></figure><p><strong>ncnn env</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4. https://github.com/RangiLyu/nanodet/blob/main/demo_ncnn/README.md</span><br><span class="line">5. ncnn test: https://blog.csdn.net/LuohenYJ/article/details/97031156</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FOUND OpenCV [error]:</span><br><span class="line">IMPORTED library can only be used with the INTERFACE keyword of</span><br><span class="line">  target_link_libraries</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Dreamacro/clash/releases" target="_blank" rel="noopener">clashamd</a><br><a href="https://proxy-switchyomega.com/download/" target="_blank" rel="noopener">switchyomega.crx</a> .crx -&gt; .zip -&gt; drag</p><ol start="5"><li>ROS env</li></ol><p><strong>ROS env</strong><br><a href="https://github.com/RoboSense-LiDAR/rslidar_sdk" target="_blank" rel="noopener">follow this readme </a><br><a href>and ros-full-desktop</a> dependencies <em>PCL</em> and <em>YAML</em> will be incidental in this version.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## add aptkeys and update repository</span></span><br><span class="line">sudo sh -c <span class="string">'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'</span></span><br><span class="line">curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -</span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">## install ros-desktop with  included</span></span><br><span class="line">sudo apt install ros-noetic-desktop-full</span><br><span class="line"></span><br><span class="line"><span class="comment">## install dependencies</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/noetic/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br><span class="line">sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential </span><br><span class="line"><span class="comment">## install these dependencies in case</span></span><br><span class="line">sudo apt-get install libboost-dev libpcap-dev libpcl-dev libeigen3-dev</span><br><span class="line"><span class="comment">## empy and catkin_pkg installation</span></span><br><span class="line"><span class="keyword">if</span> you are using python default , <span class="keyword">then</span> install python3-empy with apt</span><br><span class="line"><span class="keyword">if</span> you are using conda, <span class="keyword">then</span> ```pip install empy catkin_pkg</span><br></pre></td></tr></table></figure><h2 id="LiDAR-sdk-compilation-with-catkin"><a href="#LiDAR-sdk-compilation-with-catkin" class="headerlink" title="LiDAR sdk compilation with catkin"></a>LiDAR sdk compilation with catkin</h2><ol><li>mkdir [empty folder name]</li><li>move rslidar_sdk/ to [empty folder name]/src </li><li>catkin_make</li><li>source devel/setup.bash</li><li>roslaunch rslidar_sdk start.launch</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**ros1 + ros2**</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">## dependencies</span><br><span class="line">sudo apt-get install python3-colcon-common-extensions python3-flake8 python3-pip python3-pytest-cov python3-rosdep python3-setuptools python3-vcstool python3-rosdep</span><br><span class="line"></span><br><span class="line">## setting apt repo address</span><br><span class="line">sudo sh -c &apos;echo &quot;deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main&quot; &gt; /etc/apt/sources.list.d/ros2-latest.list&apos;</span><br><span class="line">update &amp; install ros-foxy-desktop</span><br><span class="line">file -- .bashrc</span><br><span class="line">source /opt/ros/foxy/setup.bash</span><br><span class="line">## test </span><br><span class="line">ros2 run demo_nodes_cpp talker</span><br><span class="line">## remove</span><br><span class="line">sudo apt remove ros-foxy-* &amp;&amp; sudo apt autoremove</span><br></pre></td></tr></table></figure><p><strong>AutowareAuto</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git</span><br><span class="line"><span class="comment">## dependices</span></span><br><span class="line">curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash</span><br><span class="line">sudo apt-get install git-lfs</span><br><span class="line"><span class="comment">## vsc import &amp; install ros external dependices</span></span><br><span class="line">vcs import &lt; autoware.auto.foxy.repos</span><br><span class="line">rosdep install -y -i --from-paths src</span><br><span class="line">git lfs pull --exclude=<span class="string">""</span> --include=<span class="string">"*"</span></span><br><span class="line"><span class="built_in">export</span> COLCON_DEFAULTS_FILE=/home/waynamigo/AutowareAuto/tools/ade_image/colcon-defaults.yaml</span><br><span class="line"><span class="comment">## compile by colcon with CUDA</span></span><br><span class="line">AUTOWARE_COMPILE_WITH_CUDA=1 colcon build --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br><span class="line"><span class="comment">## test</span></span><br><span class="line">colcon <span class="built_in">test</span></span><br><span class="line">colcon <span class="built_in">test</span>-result --verbose</span><br></pre></td></tr></table></figure><h2 id="ip-camera"><a href="#ip-camera" class="headerlink" title="ip camera"></a>ip camera</h2><p>ip -192.168.1.64<br>user: admin<br>passwd: abcd-1234<br><em>issues</em></p><ol><li>cant be recognized with reticle unplugged(bridge0)</li></ol><p>sdk lib <figure class="highlight plain"><figcaption><span>/etc/ld.so.conf``` **content**:</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">```bash</span><br><span class="line">include /etc/ld.so.conf.d/*.conf</span><br><span class="line">/home/waynamigo/Documents/HKVISION/CH-HCNetSDKV6.1.9.4_build20220413_linux64/lib</span><br><span class="line">/home/waynamigo/Documents/HKVISION/CH-HCNetSDKV6.1.9.4_build20220413_linux64/lib/HCNetSDKCom</span><br><span class="line">#/home/waynamigo/Documents/LiDAR</span><br><span class="line">/usr/local/lib</span><br></pre></td></tr></table></figure></p><h1 id="clip-task-–-paper"><a href="#clip-task-–-paper" class="headerlink" title="clip task – paper"></a>clip task – paper</h1><ol><li>CLIP4Hashing: Unsupervised Deep Hashing for Cross-Modal Video-Text Retrieval. ICMR</li><li>Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP. CoRR abs/2107.12518 (2021)</li></ol><h1 id="clip-retrieval"><a href="#clip-retrieval" class="headerlink" title="clip retrieval"></a>clip retrieval</h1><ol><li>Conditioned and composed image retrieval combining and partially fine-tuning CLIP-based features. CVPR Workshops 2022: 4955-4964</li><li>VideoCLIP: A Cross-Attention Model for Fast Video-Text Retrieval Task with Image CLIP. ICMR 2022: 29-33</li><li>Extending CLIP for Category-to-Image Retrieval in E-Commerce. ECIR (1) 2022: 289-303</li><li>Animating Images to Transfer CLIP for Video-Text Retrieval. SIGIR 2022: 1906-1911</li><li>X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval. CoRR abs/2207.07285 (2022)</li></ol><h2 id="cloud-native"><a href="#cloud-native" class="headerlink" title="cloud native"></a>cloud native</h2><h3 id="kv-engine"><a href="#kv-engine" class="headerlink" title="kv engine"></a>kv engine</h3><p><a href="https://www.cs.utah.edu/~lifeifei/papers/polardbserverless-sigmod21.pdf" target="_blank" rel="noopener">polarDB</a><br><a href="https://zhuanlan.zhihu.com/p/103600699" target="_blank" rel="noopener">baidu Atlas</a><br>[alibaba competition]<a href="https://tianchi.aliyun.com/competition/entrance/531979/information" target="_blank" rel="noopener">https://tianchi.aliyun.com/competition/entrance/531979/information</a></p><h2 id="go-project-structure"><a href="#go-project-structure" class="headerlink" title="go project structure"></a>go project structure</h2><p><a href="https://blog.csdn.net/weixin_44757863/article/details/120349003" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44757863/article/details/120349003</a></p><h2 id="RPC-RMI"><a href="#RPC-RMI" class="headerlink" title="RPC RMI"></a>RPC RMI</h2><p><strong>RMI</strong> stands for Remote Method Invocation, is a similar to PRC but it supports object-<br>oriented programming which is the java’s feature.<br><strong>RPC</strong> RPC(Remote Procedure Call，远程过程调用)是一种计算机通信协议，允许调用不同进程空间的程序。RPC 的客户端和服务器可以在一台机器上，也可以在不同的机器上。程序员使用时，就像调用本地程序一样，无需关注内部的实现细节。</p><p>不同的应用程序之间的通信方式有很多，比如浏览器和服务器之间广泛使用的基于 HTTP 协议的 Restful API。与 RPC 相比，Restful API 有相对统一的标准，因而更通用，兼容性更好，支持不同的语言。HTTP 协议是基于文本的，一般具备更好的可读性。但是缺点也很明显：</p><ul><li>Restful 接口需要额外的定义，无论是客户端还是服务端，都需要额外的代码来处理，而 RPC 调用则更接近于直接调用。</li><li>基于 HTTP 协议的 Restful 报文冗余，承载了过多的无效信息，而 RPC 通常使用自定义的协议格式，减少冗余报文。</li><li>RPC 可以采用更高效的序列化协议，将文本转为二进制传输，获得更高的性能。</li><li>因为 RPC 的灵活性，所以更容易扩展和集成诸如注册中心、负载均衡等功能。</li></ul><p>命令式编程(Imperative/procedual)、声明式编程(Declarative)和函数式编程(Functional)</p><h2 id="RPC-gRPC"><a href="#RPC-gRPC" class="headerlink" title="RPC gRPC"></a>RPC gRPC</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ROS, Autoware编译, rk3399开发板在实验室网络设置等的一些备忘&lt;/p&gt;
    
    </summary>
    
      <category term="PointCloud" scheme="http://waynamigo.github.io/categories/PointCloud/"/>
    
    
      <category term="PointCloud" scheme="http://waynamigo.github.io/tags/PointCloud/"/>
    
      <category term="Embedded" scheme="http://waynamigo.github.io/tags/Embedded/"/>
    
      <category term="ROS" scheme="http://waynamigo.github.io/tags/ROS/"/>
    
      <category term="Autoware" scheme="http://waynamigo.github.io/tags/Autoware/"/>
    
  </entry>
  
  <entry>
    <title>激光体积估算业务Solution?</title>
    <link href="http://waynamigo.github.io/2022/08/24/2022-08-24-%E6%BF%80%E5%85%89%E6%89%AB%E6%8F%8F%E7%9A%84%E5%A0%86%E4%BD%93%E4%BD%93%E7%A7%AF%E4%BC%B0%E7%AE%97/"/>
    <id>http://waynamigo.github.io/2022/08/24/2022-08-24-激光扫描的堆体体积估算/</id>
    <published>2022-08-23T16:00:00.000Z</published>
    <updated>2023-01-04T06:49:21.842Z</updated>
    
    <content type="html"><![CDATA[<p>通过激光进行堆体体积估算</p><a id="more"></a><h2 id="堆体体积估算"><a href="#堆体体积估算" class="headerlink" title="堆体体积估算"></a>堆体体积估算</h2><p>LiDAR场区配置：两个方案</p><h3 id="Solution1"><a href="#Solution1" class="headerlink" title="Solution1"></a>Solution1</h3><p>多个LiDAR固定后，点云拼接建图（例如三个LiDAR探测面覆盖整个料场）</p><h3 id="Solution2"><a href="#Solution2" class="headerlink" title="Solution2"></a>Solution2</h3><p>LiDAR 云台移动扫描建图（云台扫描2D切面叠加，积分计算体积）</p><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>LiDAR点云数据格式 rx,ry,rz,ro,r,g,b,s(反射强度)<br><strong>功能需求</strong><br>1) 料场盘库重量估算。激光雷达扫描料堆得到点云数据后，LiDAR根据点云数据提供料堆三维模型尺寸，切片法计算出料堆体积，通过经验密度值计算出料堆重量。<br>2) 料堆位置坐标信息。以获取每一类料堆的类别和位置，配合皮带入库位置选择及抓取出库，并可供用户实时查看库存情况。<br>3) 可视化场堆信息。根据各存料车间需求进行定制化服务，利用车间尺寸图纸划分点云地图区域，并反馈给车间管理人员实时查看现场库存情况。<br><strong>方案</strong>：<br>由移动云台带动激光雷达进行料堆扫描，或根据固定的LiDAR探测面获取的点云配准后，对料堆进行点云拼接，进行完整的建图，以准确获取料堆的斜立面信息与完整的料堆顶面信息。<br><strong>难点</strong><br>难点1，点云去噪：三维激光扫描过程中不可避免地会获得大量的噪声点云。<br>        包括漂移点、孤立点、冗余点、混杂点等)的存在不仅增加了数据量，而且会严重影响点云质量和后续矿堆体积量测<br>难点2. 不同颜色堆垛对LiDAR点云扫描的误差影响：不同颜色对光的反射是不同的，黑色反射比最小约为10%，扫描距离是最小的，如果测的是白色物体，反射比约为80%，扫描距离就比黑色范围大。<br>难点3. 误差：本方案通过切片法快速计算体积。采用拟合出的基准面（地面），根据选取的待测区域对基准面以上的3D区域，进行高精度体积测算，并结合经验密度，积分计算出重量。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过激光进行堆体体积估算&lt;/p&gt;
    
    </summary>
    
      <category term="PointCloud" scheme="http://waynamigo.github.io/categories/PointCloud/"/>
    
    
      <category term="PointCloud" scheme="http://waynamigo.github.io/tags/PointCloud/"/>
    
      <category term="Notes" scheme="http://waynamigo.github.io/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>跨模态检索论文阅读合集</title>
    <link href="http://waynamigo.github.io/2021/12/29/2021-12-29-%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%90%88%E9%9B%86/"/>
    <id>http://waynamigo.github.io/2021/12/29/2021-12-29-跨模态检索论文阅读合集/</id>
    <published>2021-12-28T16:00:00.000Z</published>
    <updated>2023-01-04T06:09:20.614Z</updated>
    
    <content type="html"><![CDATA[<p>阅读的跨模态检索相关论文合集</p><a id="more"></a><h1 id="2022-03-16更新"><a href="#2022-03-16更新" class="headerlink" title="2022-03-16更新"></a>2022-03-16更新</h1><p>论文的note及总结在mendeley里，缺点是不好导出，有空手动搞出来再更新。</p><h1 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h1><h2 id="数据集和benchmark"><a href="#数据集和benchmark" class="headerlink" title="数据集和benchmark"></a>数据集和benchmark</h2><p>常用的是MSCOCO和Flickr30k数据集，数据量多，且图像：文本为1:5，操作空间大。<br>对性能评估的讨论基本是围绕Recall@k展开，其中I2T的Recall普遍比</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/paperswithcode_flickr.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>paperswithcode的检索Flickr30K数据集benchmark</p><h1 id="SCAN-Stacked-Cross-Attention-for-Image-Text-Matching-CVPR-2018"><a href="#SCAN-Stacked-Cross-Attention-for-Image-Text-Matching-CVPR-2018" class="headerlink" title="SCAN:Stacked Cross Attention for Image-Text Matching-CVPR 2018"></a>SCAN:Stacked Cross Attention for Image-Text Matching-CVPR 2018</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读的跨模态检索相关论文合集&lt;/p&gt;
    
    </summary>
    
      <category term="DL" scheme="http://waynamigo.github.io/categories/DL/"/>
    
    
      <category term="CV" scheme="http://waynamigo.github.io/tags/CV/"/>
    
      <category term="Multimodal" scheme="http://waynamigo.github.io/tags/Multimodal/"/>
    
      <category term="PreTraining" scheme="http://waynamigo.github.io/tags/PreTraining/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Restoration of Non-rigidly Distorted Image</title>
    <link href="http://waynamigo.github.io/2021/12/17/2021-12-17-non-rigid_image_distorted_image_recovery/"/>
    <id>http://waynamigo.github.io/2021/12/17/2021-12-17-non-rigid_image_distorted_image_recovery/</id>
    <published>2021-12-16T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:09.599Z</updated>
    
    <content type="html"><![CDATA[<p>非刚性平面扰动图像恢复，应用：2D，3D，视频数据</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;非刚性平面扰动图像恢复，应用：2D，3D，视频数据&lt;/p&gt;
    
    </summary>
    
      <category term="DL" scheme="http://waynamigo.github.io/categories/DL/"/>
    
    
      <category term="CV" scheme="http://waynamigo.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Vison-Language-Navigation</title>
    <link href="http://waynamigo.github.io/2021/08/01/2021-08-01-vqa2vln/"/>
    <id>http://waynamigo.github.io/2021/08/01/2021-08-01-vqa2vln/</id>
    <published>2021-07-31T16:00:00.000Z</published>
    <updated>2022-10-26T04:23:52.443Z</updated>
    
    <content type="html"><![CDATA[<p>关于CVPR2021，VQA2VLN的tutorial的总结</p><a id="more"></a><p>tutorial原地址：<a href="https://vqa2vln-tutorial.github.io/" target="_blank" rel="noopener">CVPR 2021 Tutorial on “From VQA to VLN: Recent Advances in Vision-and-Language Research”</a>，本笔记总结一下在ppt里展示的VLN任务的内容，几位演讲者的视频还没看，毕竟有ppt和论文，概念理解应该不会有偏差</p><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>浏览了一下，VLN其实是算一个在人机交互大背景下的Visual+Language的研究方向，其实和之前做的三维重建+导航有点关系，总而言之是2D和3D视觉方面的东西，下面稍微总结了下，后几个ppt还没看完，后面会追加该方向的论文链接和相关内容</p><h2 id="See-Communicate-Act"><a href="#See-Communicate-Act" class="headerlink" title="See, Communicate, Act"></a>See, Communicate, Act</h2><p>机器模拟人的几种模拟方式，基本是将机器在看（视觉）、交流（文本）、行动几种模式下提高智能化，现在在See和Communicate两种模式的比较有代表性领域就是Computer Vision和Nature Language Processing，还有视觉和自然语言相结合的领域，例如Image-Understanding任务。前几年利用视觉和自然语言实现的智能化是从比较独立的模块获取的信息，比如目标检测，获取物体在图像上的(位置, 类别)，给机器做一些什么任务，比如统计xx，预测人流量等。目前用结合See-Communicate(Vision-Language)的发展比较好的方向有VQA, Captioning,Text2Image Generation等。</p><ul><li>会议上提到的这个VLN领域的理念是<em>Connecting Vision and Language to Actions</em>, 将vision(2D,3D)、language和(行动/指令)联系起来，理解复杂场景，并对输入请求做出具体行动，相当于是Video Understanding下的子任务。</li></ul><h2 id="Embodied-AI"><a href="#Embodied-AI" class="headerlink" title="Embodied AI"></a>Embodied AI</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/EmbodiedAI.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>“Embodied AI is the field for solving AI problems for virtual robots that can move, see, speak, and interact in the virtual world.” 这是在page里的一个引言，关于实体AI(暂译)，就是让机器获得 视听说、行动、理解几种功能，集成算法达到一个接近人一样的智能体，从Internet AI 过渡到 Embodied AI，甚至是给AGI，Artificial General Intelligence打基础，有一个关于Embodied AI的survey：<br><a href="https://arxiv.org/pdf/2103.04918.pdf" target="_blank" rel="noopener">Duan et al., A survey of Embodied AI: From simulators to Research Tasks, 2021</a></p><h1 id="Vision-Language-Navigation"><a href="#Vision-Language-Navigation" class="headerlink" title="Vision-Language Navigation"></a>Vision-Language Navigation</h1><p>VLN是2018年提出的一个研究领域，当时的子标题是<strong>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</strong>：在现实环境下，让机器理解基于视觉定位的导航指令。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/VLN.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>左下是人为给出的指令<br>通俗来讲VLN任务就是为了给机器人导航，有两个任务：<br>①如何理解现实场景<br>②对于给出的Instructions，如何理解并做出行动<br><strong>与3D视觉导航的区别</strong>（如自动驾驶使用的视觉SLAM、Radar等）</p><ul><li>3D-Navigation         : SLAM下从图像帧中处理得到点云判别障碍物、方位</li><li>Sensor-Navigation : 传感器获取的空间信息（Radar、Lidar等）    ，判别障碍物、方位<br>SLAM —&gt; PointCloud —&gt; direction<br>Sensor —&gt; Mesh —&gt; direction                </li><li>VL-Navigation          : Agent(摄像头)获取的视频信息，理解场景内容，并对指令做出行动，目前主要是在室内场景下研究导航任务</li></ul><p><strong>与VQA的区别</strong><br>VLN相对增加了动态视觉（相机运动）、长文本、测试集和训练集上的领域差距</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/VLN2VQA.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>domain gap指的是比如在室内场景下训练，在室外场景测试，效果可能就急剧下降，并且一个训练样本就可以是整个场景，（比如浙江大学的3D重建方法，NeuralRecon所用的Scannet数据）</p><h2 id="Indoor-VLN"><a href="#Indoor-VLN" class="headerlink" title="Indoor VLN"></a>Indoor VLN</h2><p>目前VLN主要是研究室内的导航任务，<br>室内导航任务的几种指令难度等级<br>    1. A到B的位移<br>    2.找东西（可见）<br>    3.找东西（不可见）<br>    4.向人有针对性的提问得到更详细的信息，用这些追加的信息找东西</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/IndoorVLN.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="Indoor-VLN-Challenges"><a href="#Indoor-VLN-Challenges" class="headerlink" title="Indoor VLN Challenges"></a>Indoor VLN Challenges</h2><p>Significant Appearance Variation，同一种物体有不同外观</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/c1.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>Rich Linguistic Phenomena，丰富的语境 </p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/c2.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>Less Words, More Contents，文本所能表达的东西太少</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/c3.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="VLN-Models"><a href="#VLN-Models" class="headerlink" title="VLN Models"></a>VLN Models</h2><ul><li>Seq2seq (a golden baseline)<br>  • Speaker-follower</li><li>Attention Mechanism (something must try)<br>  • EnvDrop, Self-monitoring, OAAM </li><li>Transformer (this is all you need)<br>  • PREVALENT, Recurrent-Bert</li><li>Reinforcement Learning (Add-on)<br>  • RCM, Soft Expert</li></ul><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>R2R 在214服务器上生成好了，从matterport模拟器得到的R2R任务 path distance 等<br>CVDN<br>NDH</p><h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><ul><li>Success / Oracle Success Rate (%)</li><li>Navigation Error (m)</li><li>SPL (Success weighted by Path Length) </li><li>CLS (Coverage weighted by Length Score)<br>  • Measuring fidelity to the reference path </li><li>nDTW (normalized Dynamic Time Warping) </li><li>SDTW (Success weighted by normalized Dynamic Time Warping)<h1 id="Addition"><a href="#Addition" class="headerlink" title="Addition"></a>Addition</h1></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于CVPR2021，VQA2VLN的tutorial的总结&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="http://waynamigo.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="DL" scheme="http://waynamigo.github.io/tags/DL/"/>
    
      <category term="MultiMedia" scheme="http://waynamigo.github.io/tags/MultiMedia/"/>
    
      <category term="Vision" scheme="http://waynamigo.github.io/tags/Vision/"/>
    
      <category term="NLP" scheme="http://waynamigo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Optiver Realized Volatility Prediction</title>
    <link href="http://waynamigo.github.io/2021/07/01/2021-07-01-kaggle-LGB-optiver-realized-volatility-prediction/"/>
    <id>http://waynamigo.github.io/2021/07/01/2021-07-01-kaggle-LGB-optiver-realized-volatility-prediction/</id>
    <published>2021-06-30T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:09.637Z</updated>
    
    <content type="html"><![CDATA[<p>以后写kaggle尽量都用一些实用性的算法，该面向简历编程了，论文阅读笔记之类的以后都尽量用英语写</p><a id="more"></a><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Optiver Realized Volatility Prediction Competition.<br>This kaggle project is about trying diff methods to predict the volatility of a trading floor for trading firms,The Accurate Volatility, which is essencial for their investing options.Also is an essencial data standard related to the price of underlying product.<br>IN short, We have to find the most effective approach to minus RMSPE.</p><h3 id="Given-Data"><a href="#Given-Data" class="headerlink" title="Given Data"></a>Given Data</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataset</span><br><span class="line">├── book_test.parquet</span><br><span class="line">├── book_train.parquet</span><br><span class="line">├── trade_test.parquet</span><br><span class="line">└── trade_train.parquet</span><br></pre></td></tr></table></figure><p>Each folder contains stock_id=n<br><strong>trade</strong> [‘time_id’, ‘seconds_in_bucket’, ‘price’, ‘size’, ‘order_count’]<br><strong>book</strong> [‘time_id’, ‘seconds_in_bucket’, ‘bid_price1’, ‘ask_price1’, ‘bid_price2’, ‘ask_price2’, ‘bid_size1’, ‘ask_size1’, ‘bid_size2’, ‘ask_size2’],<br><strong>train</strong> [‘stock_id’,’time_id’,’target’]<br><strong>test</strong> [‘stock_id’,’time_id’,’row_id’]</p><h3 id="financial-concepts"><a href="#financial-concepts" class="headerlink" title="financial concepts"></a>financial concepts</h3><p>show case:</p><table><thead><tr><th align="center">bid</th><th align="center">price</th><th align="center">ask</th></tr></thead><tbody><tr><td align="center"></td><td align="center">151</td><td align="center">196</td></tr><tr><td align="center"></td><td align="center">150</td><td align="center">189</td></tr><tr><td align="center"></td><td align="center">149</td><td align="center">148</td></tr><tr><td align="center"></td><td align="center">148</td><td align="center">221</td></tr><tr><td align="center">251</td><td align="center">147</td><td align="center"></td></tr><tr><td align="center">351</td><td align="center">146</td><td align="center"></td></tr><tr><td align="center">300</td><td align="center">145</td><td align="center"></td></tr><tr><td align="center">20</td><td align="center">144</td><td align="center"></td></tr><tr><td align="center"><em>1.Content of an order book</em></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">- A list of buy or sell records sorted by price, which lists the number of shares being bid on or offered at each price point.</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">- in the case of given data,’bid’ means <em>How many shares the Buyer want to buy</em> , ‘ask’ means <em>How many shares Sellers offer</em>.</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><strong>EACH order book&amp;trade book belongs to 1 kind of stock</strong></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><em>2.Trade procedure</em></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">- a TRADE HAPPENS when the shares of stock that seller S offers and buyer B bids  at the same price.</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">- B can up his/her intended price and buy the offered by S.</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><em>3.Liquidity</em></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">there’re some statistics standards for analyser to estimate the liquidity of an order book.</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">- WAP(weighted avaraged price)takes the price level and size of orders</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">$$wap = \frac{bidprice1<em>asksize1+askprice1</em>bidsize1}{asksize1+bidsize1}$$</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Code for WAP caculation, add one column as ‘wap’</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">book_parquet[<span class="string">'wap'</span>] = </span><br><span class="line">(book_parquet[<span class="string">'bid_price1'</span>] * book_parquet[<span class="string">'ask_size1'</span>] + </span><br><span class="line">book_parquet[<span class="string">'ask_price1'</span>] * book_parquet[<span class="string">'bid_size1'</span>])</span><br><span class="line">/(book_parquet[<span class="string">'bid_size1'</span>]+ book_parquet[<span class="string">'ask_size1'</span>])</span><br></pre></td></tr></table></figure></td><td align="center"></td><td align="center"></td></tr></tbody></table><p><em>4.Log returns</em><br>another vital standard for comparing the price of a stock in yesterday and today<br>calling $S_t$ is the price of stock at time $t$ ,the log return is $r_{t1,t2}$,<br>$$r_{t_1, t_2} = \log{\frac{S_{t_2}}{S_{t_1}}}$$<br>Noticed <strong>The host wants competitors should use WAP to compute log returns, and assuming that log returns have 0 mean</strong><br>Then the Code for LogReturn is as follows and add it to book table.<br>Additionally we should expire the NaN row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LogReturn</span><span class="params">(WAP)</span>:</span></span><br><span class="line"><span class="keyword">return</span> np.log(WAP)</span><br><span class="line">book_parquet[<span class="string">'logreturn'</span>] = LogReturn(book_parquet)</span><br><span class="line"><span class="comment">#expire NaN items</span></span><br><span class="line">book_parquet = book_parquet[~book_example[<span class="string">'log_return'</span>].isnull()]</span><br></pre></td></tr></table></figure><p><em>5.Realized Volatility</em><br>Volatility is described as ‘the annualized standard deviation of one year’s LogReturn’<br>$$\sigma = \sqrt{\sum\limits_t{r^2_{t-1,t}}}$$</p><p>For each stock data, we find that different stock have different volatility characteristics, So one column should be added as ‘stock_id’, using</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stock_id = i</span><br><span class="line">book_parquet.loc[:,<span class="string">'stock_id'</span>] = stock_id</span><br></pre></td></tr></table></figure><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>The evaluation metric is Root Mean Square Percentage Error, as:<br>$$\text{RMSPE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ((y_i - \hat{y}_i)/y_i)^2}$$<br>The formula above can be implemented as:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSPE</span><span class="params">(yhat, data)</span>:</span></span><br><span class="line">y = data.get_label()</span><br><span class="line">elements = ((y - yhat) / y) ** <span class="number">2</span></span><br><span class="line"><span class="keyword">return</span> float(np.sqrt(np.sum(elements) / len(y)))</span><br></pre></td></tr></table></figure><h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><p>I looked through the Discussion board, found most are using <em>XGBoost</em> and <em>LightGBT</em>, I get begin from  DataProcessing module and the baseline is implemented with XGBoost, LightGBT will be done later.</p><h3 id="data-processing"><a href="#data-processing" class="headerlink" title="data processing"></a>data processing</h3><p>First check how we should process the parquet file.<br><strong>Parquet</strong>  is a <a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS" target="_blank" rel="noopener">columnar storage</a> format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.<br>Hoster provided code for process the columnar file.<br><strong>and, I’m goin to try to run this method and data on Spark</strong>, The code will be release later on github.<br>Process code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="APIs"><a href="#APIs" class="headerlink" title="APIs"></a>APIs</h2><p>you can use <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html" target="_blank" rel="noopener">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html</a> to see the feature importance of your model.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以后写kaggle尽量都用一些实用性的算法，该面向简历编程了，论文阅读笔记之类的以后都尽量用英语写&lt;/p&gt;
    
    </summary>
    
      <category term="kaggle" scheme="http://waynamigo.github.io/categories/kaggle/"/>
    
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="kaggle" scheme="http://waynamigo.github.io/tags/kaggle/"/>
    
      <category term="Boosting" scheme="http://waynamigo.github.io/tags/Boosting/"/>
    
      <category term="xgboost" scheme="http://waynamigo.github.io/tags/xgboost/"/>
    
      <category term="GBDT" scheme="http://waynamigo.github.io/tags/GBDT/"/>
    
  </entry>
  
  <entry>
    <title>统计学习笔记(后篇)</title>
    <link href="http://waynamigo.github.io/2021/07/01/2021-07-01-statics_note/"/>
    <id>http://waynamigo.github.io/2021/07/01/2021-07-01-statics_note/</id>
    <published>2021-06-30T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:09.705Z</updated>
    
    <content type="html"><![CDATA[<p>接上一篇统计学习笔记，最近在做kaggle，用到了几种Boost，有机会把以前笔记剩余部分的补上了。</p><a id="more"></a><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting类方法是将N个弱学习算法构建成一个相当于强学习算法的方法。例如用一套基本分类器按照不同的权重组合成一个强分类器，这些基本分类器之间有依赖关系，构成的最终分类器可以达到复杂的强分类器达到的效果，并且可以使用并行处理，时间效率比复杂模型更快。</p><p>在这里重点记录AdaBoost和XGBoost两个模型，也是实际应用中广泛使用的方法，最后会讨论效果和时间性能。</p><p>sklearn的ensemble里封装了<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble" target="_blank" rel="noopener">AdaBoostClassifier/AdaBoostRegressor</a>。</p><h3 id="Boosting类的算法原理"><a href="#Boosting类的算法原理" class="headerlink" title="Boosting类的算法原理"></a>Boosting类的算法原理</h3><ol><li>boosting的基本思想是选取弱学习器，可以是各种算法组合在一起用，也可用n个相同的算法，设定不同超参数。</li><li>组合方式，并行方式（Voting），串行（Cascading)。区别在于，并行方式是每个弱学习器给自己的结果，串行方式是第i+1个弱学习器只在第i个的置信度结果不够高时，对上一级的结果进行预测。</li></ol><h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><h3 id="Adaboost解决的问题"><a href="#Adaboost解决的问题" class="headerlink" title="Adaboost解决的问题"></a>Adaboost解决的问题</h3><p>Adaboosting如何实现自适应<br>(适应弱分类器各自的训练误差率)，由权值D实现，那么：</p><ol><li>每一轮如何改变训练数据的权值或者概率分布？</li></ol><ul><li><strong>提高被前一轮弱分类器错误分类的样本的权值，降低被正确分类样本的权值，权值改变依赖自己的权值更新策略，不同任务和训练集一般不同，必要时需要设计自己的更新策略</strong></li></ul><ol start="2"><li>如何将N个弱学习模型组合成一个强学习模型</li></ol><ul><li><strong>Adaboost采取加权voting，准确率误差小的学习器权重高，误差大的权重小</strong></li></ul><h3 id="Adaboost算法流程"><a href="#Adaboost算法流程" class="headerlink" title="Adaboost算法流程"></a>Adaboost算法流程</h3><p>&lt;!—&gt;<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/adaboost-algorithm.png" alt title>                </div>                <div class="image-caption"></div>            </figure><br>&lt;—&gt;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">权重参数D，弱分类器个数</span><br></pre></td></tr></table></figure><p>算法机制很容易理解，就拿李航那本书上的-1和1做分类的问题，拆分成</p><ol><li>加法模型$f(x)=\sum{\beta_mb(x;\gamma_m)}$</li><li>损失函数</li><li>前向分布</li></ol><p>1.加法模型是指：有一系列基函数，其中基函数的变量$x$和某种系数$\gamma$，设有m次迭代，则每次迭代产生系数$\gamma_m$和基函数的系数$\beta_m$，产生$\beta_mb(x;\gamma_m)$，我们也可以把这个基函数成为分类器$G$，最终模型也可以写成$f(x)=\sum{\beta_mb(x;\gamma_m)}=\sum{\alpha_mG(x)_m}$</p><p>2.损失函数：由于含supervision，我们采取的损失函数可以采取大部分误差项或CE等损失函数<br>3.前向分布计算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">func[]</span><br><span class="line">for i in range(m):</span><br><span class="line">    (beta,gamma) =</span><br><span class="line">        argmin(sum(Loss(y,f(x)+beta*G(x,gamma))))</span><br><span class="line">    func[] = func[] + beta*G()</span><br><span class="line">res = func[](x)</span><br></pre></td></tr></table></figure><p>在argmin的操作里，涉及到系数的计算，首先我们需要每次迭代时计算分类误差（以第m次迭代为例）$err_m=\sum{w_{m,i}I(G_m(x)!=y)}$，那么$\beta_m=0.5\ln{\frac{1-err_m}{err_m}}$,更新样本权重$D,w_{m+1,i}=\frac{w_{m,i}}{Z_m}exp(-\beta_mG_m(x)y_i),Z=\sum{-\beta_mG_m(x)y_i}$,为归一化因子。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">问题<span class="number">1.</span>为什么不能用神经网络一类的强分类器。</span><br><span class="line">因为adaboost是梯度算法，若果弱分类器太强，则会陷入局部最优。</span><br><span class="line"></span><br><span class="line">问题<span class="number">2.</span>当有不支持加权重的弱分类器时，如何解决。</span><br><span class="line">resampling样本</span><br><span class="line"></span><br><span class="line">问题<span class="number">3.</span>权重的作用</span><br><span class="line"><span class="number">1.</span>计算弱分类器的加权系数，训练弱分类器G</span><br><span class="line"><span class="number">2.</span></span><br><span class="line">问题<span class="number">4.</span>与random forest的区别</span><br><span class="line"><span class="number">1.</span>随机森林的每棵树独立做出决策，训练时没有先后顺序</span><br><span class="line"><span class="number">2.</span>adaboost的tree中，每个G以上一个的结果为输</span><br></pre></td></tr></table></figure><h2 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h2><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><p>1.高斯混合分布，<br>2.隐变量，从高斯混合分布产生的变量</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上一篇统计学习笔记，最近在做kaggle，用到了几种Boost，有机会把以前笔记剩余部分的补上了。&lt;/p&gt;
    
    </summary>
    
      <category term="数学" scheme="http://waynamigo.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="Boost" scheme="http://waynamigo.github.io/tags/Boost/"/>
    
      <category term="AdaBoost" scheme="http://waynamigo.github.io/tags/AdaBoost/"/>
    
      <category term="XGBoost" scheme="http://waynamigo.github.io/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>3D重建及其深度学习方法的相关论文解析</title>
    <link href="http://waynamigo.github.io/2021/05/13/2021-05-13-3D%E8%A7%86%E8%A7%89_%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA_%E5%AE%9E%E4%B9%A0/"/>
    <id>http://waynamigo.github.io/2021/05/13/2021-05-13-3D视觉_三维重建_实习/</id>
    <published>2021-05-12T16:00:00.000Z</published>
    <updated>2022-08-23T09:02:06.871Z</updated>
    
    <content type="html"><![CDATA[<p>最近实习在读浙大最近的NeuralRecon，里面涉及到一些3D点云处理的数学方法和3DVison的深度学习Tricks，主要包括TSDF算法，特征点提取的SIFT、ORB两种算法，及稀疏卷积等，以及SLAM(simultaneous localization and mapping)基础，从《视觉SLAM十四讲》（高翔等）学习基础。双目算法现在已经比较成熟，目前自己工作只涉及单目相机。</p><a id="more"></a><h2 id="SLAM框架"><a href="#SLAM框架" class="headerlink" title="SLAM框架"></a>SLAM框架</h2><h3 id="视觉里程计（Visual-Odometry）"><a href="#视觉里程计（Visual-Odometry）" class="headerlink" title="视觉里程计（Visual Odometry）"></a>视觉里程计（Visual Odometry）</h3><p>目前项目的硬件设备由单目相机获取信息，进而进行姿态估计、深度估计等计算。<br>简单来说，VO是由相邻两张图片间像素的位置关系估计相机的位置，</p><h4 id="坐标变换-旋转矩阵"><a href="#坐标变换-旋转矩阵" class="headerlink" title="坐标变换 旋转矩阵"></a>坐标变换 旋转矩阵</h4><p>求相机坐标系（o）到世界坐标系（w）下的旋转矩阵$R^o_w$，进行欧式变换可以将o下的向量$p_o$ 转换到w下向量$p_w$<br>求出刚体旋转矩阵$R^o_w$，那么w下向量$p_w$左乘R就可以转化到$p_o$:<br>$p_o = R^o_w  \cdot\ p_w$<br>同理如果两个坐标系下的旋转矩阵可以得到<br>$$m = $$<br><strong>eg.</strong> 下面是由w到o1 和o2两个旋转矩阵传递得到的o1 -&gt; o2的旋转矩阵<br>设两个相机坐标系下o1,o2对应的三个点a b c，d e f，各获得两个向量 $m1,n1$,$m2,n2$<br>分别构建出该点集合所在的坐标系方程，求解得到世界坐标到o1 , o2的旋转坐标$R_1,R_2$， 则有：世界坐标系向量$m2 = R_1^T \cdot R_2 \cdot m1$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def RigidBody_Transform(p=np.zeros((3,3),dtype=float), q=np.zeros((3,3),dtype=float)):</span><br><span class="line">    # 世界坐标到 O1的旋转矩阵 ，x = x / ||x|| 单位化</span><br><span class="line">    x = (p[1,:] - p[0,:]) / np.linalg.norm(p[1,:] - p[0,:]) </span><br><span class="line">    y = (p[2,:] - p[1,:]) - np.inner(np.inner((p[2,:] - p[1,:]),x), x)</span><br><span class="line">    y = y / np.linalg.norm(y) # y_bar = y / ||y|| # 单位化第二行</span><br><span class="line">    print(&quot;x&quot;,x)</span><br><span class="line">    print(&quot;y&quot;,y)</span><br><span class="line">    z = np.cross(x, y) #叉乘</span><br><span class="line">    print(&quot;z&quot;,z)</span><br><span class="line">    rotate_matrix_w2o1 = np.array([x, y, z])</span><br><span class="line">    # 世界坐标到  O2的转转矩阵</span><br><span class="line">    x_new = (q[1,:] - q[0,:]) / np.linalg.norm(q[1,:] - q[0,:]) </span><br><span class="line">    y_new = (q[2,:] - q[1,:]) - np.inner(np.inner((q[2,:] - q[1,:]),x_new), x_new)</span><br><span class="line">    y_new = y_new / np.linalg.norm(y_new)</span><br><span class="line">    # 这里要注意，叉积（cross product）和 外积（outer product）不一样</span><br><span class="line">    # ps:国内教材讲的是叉积和外积一样</span><br><span class="line">    # np.cross算叉积 ，np.outer算外积</span><br><span class="line">    z_new = np.cross(x_new, y_new) #这要计算的是叉积（只有三维空间有意义，就是右手系的那个）</span><br><span class="line">    rotate_matrix_w2o2 = np.array([x_new, y_new, z_new])</span><br><span class="line">    return rotate_matrix_w2o1,rotate_matrix_w2o2, (rotate_matrix_w2o1.T * rotate_matrix_w2o2)</span><br><span class="line"></span><br><span class="line">#p = np.array([(-47.34,-18.71,-155.02), (-73.64,-29.82,-210.88), (-64.88,-36.77,-216.15)])</span><br><span class="line">#p = np.array([(-4.34,-36.71,51), (-30,25.5,-4), (-21,18,-10.15)])</span><br><span class="line">#q = np.array([(-40,25.5,6), (-30,25.5,-4), (15.01,55.19,22.818)])</span><br><span class="line">p = np.array([(0,1,0), (0,0,0), (0,0,1)])</span><br><span class="line">q = np.array([(0,-1,0), (0,0,0), (0,0,-1)])</span><br><span class="line">m1,m2,rotate_matrix = RigidBody_Transform(p, q)</span><br><span class="line">print(&quot;rotate_matrix is:\n&quot;)</span><br><span class="line">print(rotate_matrix)</span><br></pre></td></tr></table></figure><h4 id="旋转向量"><a href="#旋转向量" class="headerlink" title="旋转向量"></a>旋转向量</h4><p>上面的旋转矩阵表示具有局限性，原因是求出的矩阵必须是正交阵，优化时比较困难，并且计算量比较大，需要进行矩阵运算，一次运算需要9次浮点乘法，所以又提出一个用<strong>旋转角和旋转轴</strong>表示一个<strong>旋转向量</strong>的描述旋转的方法。同时，旋转向量也可以转换成旋转矩阵:<br>由<strong>罗德里格斯公式(Rodrigus’ Formula)</strong>，n_r 即n^，表示向量n到n对应的反对称矩阵的转换符，计算如下：</p><p>$a \times b=\begin{Vmatrix} e_1&amp;e_2&amp;e_3\\a_1&amp;a_2&amp;a_3\\b_1&amp;b_2&amp;b_3 \end{Vmatrix}=\begin{bmatrix} a_2b_3-a_3b_2\\a_3b_1-a_1b_3\\a_1b_2-a_2b_1 \end{bmatrix}=\begin{bmatrix}0&amp;-a_3&amp;a_2\\a_3&amp;0&amp;-a_1\\ -a_2&amp;a_1&amp;0\end{bmatrix} \cdot b = a$^$b$<br>上面的$a$^ 表示其对应的反对称矩阵<br>$R = cos \theta I  + (1-cos\theta) n \cdot n^T + sin\theta n$^<br>求转角$\theta$，可以：<br>$tr(R)= \cos\theta tr(I) +(1-\cos\theta)tr(n \cdot n^T) +\sin\theta tr(n$^$)$<br>$\quad\quad =3\cos\theta +(1-\cos\theta) = 1+2\cos\theta$<br>求出$\theta = \arccos \frac{tr(R)-1}{2}$</p><h3 id="回环检测（Loop-Closure-Detection）"><a href="#回环检测（Loop-Closure-Detection）" class="headerlink" title="回环检测（Loop Closure Detection）"></a>回环检测（Loop Closure Detection）</h3><p>判断镜头是否到达过先前位置，和后端（优化）解决因里程计每次计算相邻两张图片的位置关系，每次前后误差叠加出现的<strong>漂移</strong>问题，简而言之就是校正。</p><h3 id="后端优化（非线性）"><a href="#后端优化（非线性）" class="headerlink" title="后端优化（非线性）"></a>后端优化（非线性）</h3><p>接受VO获得的相机位姿、回环检测</p><h3 id="建图"><a href="#建图" class="headerlink" title="建图"></a>建图</h3><p>还没整理好</p><h2 id="3D点云模型"><a href="#3D点云模型" class="headerlink" title="3D点云模型"></a>3D点云模型</h2><h3 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h3><p>相机模型得到的相机内参(camera_intrinsic_perview)一般为一张图得到一个<br>通过两种模型<strong>针孔模型（PINHOLE，还有放射模型RADIAL）</strong>，<strong>畸变模型</strong>两种实现内参计算</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近实习在读浙大最近的NeuralRecon，里面涉及到一些3D点云处理的数学方法和3DVison的深度学习Tricks，主要包括TSDF算法，特征点提取的SIFT、ORB两种算法，及稀疏卷积等，以及SLAM(simultaneous localization and mapping)基础，从《视觉SLAM十四讲》（高翔等）学习基础。双目算法现在已经比较成熟，目前自己工作只涉及单目相机。&lt;/p&gt;
    
    </summary>
    
      <category term="3DPointCloud" scheme="http://waynamigo.github.io/categories/3DPointCloud/"/>
    
    
      <category term="DeepLearning" scheme="http://waynamigo.github.io/tags/DeepLearning/"/>
    
      <category term="3DVision" scheme="http://waynamigo.github.io/tags/3DVision/"/>
    
      <category term="3DPointCloud" scheme="http://waynamigo.github.io/tags/3DPointCloud/"/>
    
      <category term="SLAM" scheme="http://waynamigo.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>NeuralRecon for 3D reconstruction in real-time</title>
    <link href="http://waynamigo.github.io/2021/05/11/2021-05-11-summary_neucon/"/>
    <id>http://waynamigo.github.io/2021/05/11/2021-05-11-summary_neucon/</id>
    <published>2021-05-10T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:11.472Z</updated>
    
    <content type="html"><![CDATA[<p>图像pair 提取特征点算法<br>SIFT<br>SURF<br>ORB</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像pair 提取特征点算法&lt;br&gt;SIFT&lt;br&gt;SURF&lt;br&gt;ORB&lt;/p&gt;
    
    </summary>
    
      <category term="3D vision" scheme="http://waynamigo.github.io/categories/3D-vision/"/>
    
    
      <category term="3DVision Projection TSDF" scheme="http://waynamigo.github.io/tags/3DVision-Projection-TSDF/"/>
    
  </entry>
  
  <entry>
    <title>DenseDescriptor for SfM Datasset Preparation</title>
    <link href="http://waynamigo.github.io/2021/04/19/2021-04-20-DenseDescriptor/"/>
    <id>http://waynamigo.github.io/2021/04/19/2021-04-20-DenseDescriptor/</id>
    <published>2021-04-18T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:09.741Z</updated>
    
    <content type="html"><![CDATA[<p>一些单目三维重建的概念，及DepthEstimation代码的阅读</p><a id="more"></a><h2 id="4-19日-4月21日"><a href="#4-19日-4月21日" class="headerlink" title="4-19日-4月21日"></a>4-19日-4月21日</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Colmap提取的数据，SFMDataset用来初始化</span><br><span class="line"></span><br><span class="line">ColMap提取SFM数据，作为训练数据集，读取及处理方式</span><br></pre></td></tr></table></figure><h2 id="SLAM和ColMap两个生成数据-输入到DenseDescriptor的兼容性"><a href="#SLAM和ColMap两个生成数据-输入到DenseDescriptor的兼容性" class="headerlink" title="SLAM和ColMap两个生成数据,输入到DenseDescriptor的兼容性"></a>SLAM和ColMap两个生成数据,输入到DenseDescriptor的兼容性</h2><p>class:<br>SFMDataset</p><ul><li>Format</li></ul><hr><p>image_file_names        拆好的图像序列，有序<br>folder_list             data里面的文件夹train/data/1 train/data/2<br>adjandance_range        1 50 邻接范围,控制1-50的随机增量<br>image_downsampling      2.5 图像下采样倍数 resize到 原来的2.5x<br>network_downsampling    64 for downsample and crop mask的参数<br>inlier_percentage       0.99 阈值ground truth<br>load_intermediate_data  True/False 是否加载预计算数据，存在precompute的pickle文件里precompute.pkl<br>intermediate_data_root  precompute文件⬆️的path<br>sampling_size           10<br>heatmap_sigma        5.0 热图参数，用于generate_heatmap_from_locations,生成训练的sourcemap和targetmap<br>pre_workers             4<br>visible_interval         可视化间隔，，用在overlap点云的函数里，和读取colmapresult的函数一起预处理，避免点云密集，可以调整该参数控制稀疏程度。</p><h2 id="num-iter-每个epoch的迭代次数，训练的时候在看"><a href="#num-iter-每个epoch的迭代次数，训练的时候在看" class="headerlink" title="num_iter              每个epoch的迭代次数，训练的时候在看"></a>num_iter              每个epoch的迭代次数，训练的时候在看</h2><ul><li>precompute.pkl 按作者计算的程序来吧，反正按路径来就没问题</li></ul><hr><pre><code>      crop_positions_per_seq    selected_indexed_per_seqvisible_view_indexes_per_seq     point_cloud_per_seq    intrinsic_matrix_per_seq       mask_boundary_per_seq  view_indexes_per_point_per_seq      extrinsics_per_seq      projection_per_seq    clean_point_list_per_seq    image_downsampling   //这三个是    network_downsampling    inlier_percentage    // 符合ground trueth的阈值     estimated_scale_per_seq</code></pre><hr><h3 id="使用tensorrt生成engine进行推理"><a href="#使用tensorrt生成engine进行推理" class="headerlink" title="使用tensorrt生成engine进行推理"></a>使用tensorrt生成engine进行推理</h3><p><a href="https://zhuanlan.zhihu.com/p/351426774" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/351426774</a><br>c++ 写法、思路如下</p><ul><li><p>先将pytorch的Network先转成onnx模型。<br>如果使用DataParallel进行多GPU训练的话，需要注意节点前面的Module.<br>注意版本，某些函数是onnx默认运算符集不支持的函数，比如forbenius norm，只能转成Aten运算符，Aten运算符竟然没找到很好的文档，为了避免风险升级pytorch到 1.6，将运算符集合版本导出为11，支持了现在的大多数函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">code here</span><br></pre></td></tr></table></figure></li><li><p>导出onnx在netron看一下，没问题就可以开始用C++转Trt模型，主要包括加载、解析onnx，序列化两个操作进行</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> trtEngineName = <span class="string">"out.engine"</span>;</span><br><span class="line">sammple::Logger glogger; </span><br><span class="line">nvinfer1::IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());<span class="comment">//createInferBuilder(ILogger&amp; logger);</span></span><br><span class="line">INetWorkDefinition* network = builder-&gt;createNetWorkV2(maxBatchSize);<span class="comment">//</span></span><br><span class="line">IBuilderConfig*     config  = builder-&gt;createBuilderConfig();</span><br><span class="line"><span class="keyword">auto</span> parser  =nvonnxparser::createParser(*network,gLogger.getTRTLogger());<span class="comment">// a parser for onnx</span></span><br><span class="line"></span><br><span class="line">builder-&gt;setMaxWorkspaceSize(<span class="number">1</span>_GiB);<span class="comment">//NVIDIA document claims "lets TensorRT pick any algorithm available."</span></span><br><span class="line">config-&gt;setMaxWorkspaceSize(<span class="number">1</span>_GiB);</span><br><span class="line"></span><br><span class="line">builder-&gt; setFp16Mode(gArgs.runInFp16);<span class="comment">//two inference mode, FP16 and Int8, Float16 is okay</span></span><br><span class="line"></span><br><span class="line">samplesCommon::enableDLA(builder, config, gArgs.useDLACore);<span class="comment">// DLA is to accelerate some layer // DALI to accelerate data reading</span></span><br><span class="line"></span><br><span class="line">ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);<span class="comment">// build cudaengine of "NvInferRuntime.h"</span></span><br><span class="line"></span><br><span class="line">IHostMemory* trtModel = <span class="literal">nullptr</span>;<span class="comment">// init stream as null point</span></span><br><span class="line">trtModel = engine -&gt;serialize(); <span class="comment">// serialize the onnx model</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="function">ofstream <span class="title">ofs</span><span class="params">(trtEngineName.c_str(), <span class="built_in">std</span>::ios::out | <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line">ofs.write((<span class="keyword">char</span>*)(trtModel-&gt;data()), trtModel-&gt;size());</span><br><span class="line">ofs.close();</span><br></pre></td></tr></table></figure></li><li><p>上一步导出的模型为out.engine，下一步加载该TRT model（或CudaEngine）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TensorRT</span> &#123;</span></span><br><span class="line">IExecutionContext* context;</span><br><span class="line">ICudaEngine* engine;</span><br><span class="line">IRuntime* runtime;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">TensorRT* <span class="title">LoadNet</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* trtFileName)</span></span>&#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="function">ifstream <span class="title">t</span><span class="params">(trtFileName, <span class="built_in">std</span>::ios::in | <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">stringstream</span> tempStream;</span><br><span class="line">tempStream &lt;&lt; t.rdbuf();</span><br><span class="line">t.close();</span><br><span class="line">DebugP(<span class="string">"TRT File Loaded"</span>);</span><br><span class="line"></span><br><span class="line">tempStream.seekg(<span class="number">0</span>, <span class="built_in">std</span>::ios::end);</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> modelSize = tempStream.tellg();</span><br><span class="line">tempStream.seekg(<span class="number">0</span>, <span class="built_in">std</span>::ios::beg);</span><br><span class="line"><span class="keyword">void</span>* modelMem = <span class="built_in">malloc</span>(modelSize);</span><br><span class="line">tempStream.read((<span class="keyword">char</span>*)modelMem, modelSize);</span><br><span class="line"></span><br><span class="line">IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line"><span class="keyword">if</span> (runtime == <span class="literal">nullptr</span>)</span><br><span class="line">&#123;</span><br><span class="line">DebugP(<span class="string">"Build Runtime Failure"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (gArgs.useDLACore &gt;= <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">runtime-&gt;setDLACore(gArgs.useDLACore);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(modelMem, modelSize, <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (engine == <span class="literal">nullptr</span>)</span><br><span class="line">&#123;</span><br><span class="line">DebugP(<span class="string">"Build Engine Failure"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IExecutionContext* context = engine-&gt;createExecutionContext();</span><br><span class="line"><span class="keyword">if</span> (context == <span class="literal">nullptr</span>)</span><br><span class="line">&#123;</span><br><span class="line">DebugP(<span class="string">"Build Context Failure"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TensorRT* trt = <span class="keyword">new</span> TensorRT();</span><br><span class="line">trt-&gt;context = context;</span><br><span class="line">trt-&gt;engine = engine;</span><br><span class="line">trt-&gt;runtime = runtime;</span><br><span class="line">DebugP(<span class="string">"Build trt Model Success!"</span>);</span><br><span class="line"><span class="keyword">return</span> trt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些单目三维重建的概念，及DepthEstimation代码的阅读&lt;/p&gt;
    
    </summary>
    
      <category term="3DPointCloud" scheme="http://waynamigo.github.io/categories/3DPointCloud/"/>
    
    
      <category term="3DPointclouod" scheme="http://waynamigo.github.io/tags/3DPointclouod/"/>
    
      <category term="DeepLearning" scheme="http://waynamigo.github.io/tags/DeepLearning/"/>
    
      <category term="DepthEstimation" scheme="http://waynamigo.github.io/tags/DepthEstimation/"/>
    
  </entry>
  
  <entry>
    <title>跨媒体检索/多模态计算 方向动态</title>
    <link href="http://waynamigo.github.io/2021/04/08/2021-04-08-%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E4%B8%8E%E6%A3%80%E7%B4%A2/"/>
    <id>http://waynamigo.github.io/2021/04/08/2021-04-08-图像视频信息提取与检索/</id>
    <published>2021-04-07T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:09.775Z</updated>
    
    <content type="html"><![CDATA[<p>写在前面：跨媒体检索方向涵盖许多任务，涉及到图像、文本、语音、视频等多种模态的数据，事实上根据项目需求，开发者可以将所需的识别、分割、生成、编码等方法集成到检索或推荐项目中。<br>本文整理了在网络上能搜集到的Baidu、Youtube、Google、Facebook检索系统和大数据架构实现方案当做参考。</p><a id="more"></a><h2 id="多模态信息检索的挑战和攻克方向"><a href="#多模态信息检索的挑战和攻克方向" class="headerlink" title="多模态信息检索的挑战和攻克方向"></a>多模态信息检索的挑战和攻克方向</h2><p>In fact, researchers and algorithm engineers in the field of information retrieval focus more on tasks such as data mining, feature representation, and analysis of user behavior. From the recent conferences  like SIGIR and ACMMM, some research directions retrieved are as follows:<br><em>2021 SIGIR</em><br>Bias and counterfactual learning<br>Recommendation<br>Searching and Ranking<br>Social Aspects<br>Knowledge Structures<br>Question Answering<br>Sequences and Sessions<br>Adversarial Information Retrieval<br>Multi-modal Information Retrieval<br>MultiMedia Information Retrieval<br>Multi-modal Fusion and Embedding<br><em>2020 SIGIR</em></p><p>As noticed, the main modalities are visual, texual, and acoustic.  The challanges lie on <strong>Multimodal Fusion</strong>. Many problems in engeneer often comes to: <em>Infor mation loss</em>, <em>hierachical structure</em></p><h2 id="transductive-learning"><a href="#transductive-learning" class="headerlink" title="transductive learning"></a>transductive learning</h2><p>optimal latent space, can maintance original intrinsic characteristics of microvideo in original space</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;写在前面：跨媒体检索方向涵盖许多任务，涉及到图像、文本、语音、视频等多种模态的数据，事实上根据项目需求，开发者可以将所需的识别、分割、生成、编码等方法集成到检索或推荐项目中。&lt;br&gt;本文整理了在网络上能搜集到的Baidu、Youtube、Google、Facebook检索系统和大数据架构实现方案当做参考。&lt;/p&gt;
    
    </summary>
    
      <category term="Multimodal" scheme="http://waynamigo.github.io/categories/Multimodal/"/>
    
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="DL" scheme="http://waynamigo.github.io/tags/DL/"/>
    
      <category term="MultiMedia" scheme="http://waynamigo.github.io/tags/MultiMedia/"/>
    
      <category term="Recommendation" scheme="http://waynamigo.github.io/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>CPP面试</title>
    <link href="http://waynamigo.github.io/2021/03/26/2021-03-26-cpp%E9%9D%A2%E8%AF%95/"/>
    <id>http://waynamigo.github.io/2021/03/26/2021-03-26-cpp面试/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:11.763Z</updated>
    
    <content type="html"><![CDATA[<p>持续更新</p><a id="more"></a><h2 id="概念性区分"><a href="#概念性区分" class="headerlink" title="概念性区分"></a>概念性区分</h2><p><strong>1.C和C++的区别</strong><br>C面向过程，C++面向对象<br>C的内存管理使用malloc free，C++还可以使用new delete<br>C不支持函数重载，C++支持函数重载<br>C没有引用，C++可以用引用<br><strong>堆和栈的区别</strong><br>stack编译器自动分配和释放，自底向上的数据结构<br>heap需要由程序员手动new delete，会产生外部碎片，是自上到下的数据结构<br><strong>c++中不能被继承的成员函数</strong><br>析构函数和构造函数<br><strong>const</strong><br>定义常量<br>修饰函数参数和函数返回值</p><h2 id="修饰函数定义体，函数为类的成员函数，const修饰后的成员函数不修改成员变量的值"><a href="#修饰函数定义体，函数为类的成员函数，const修饰后的成员函数不修改成员变量的值" class="headerlink" title="修饰函数定义体，函数为类的成员函数，const修饰后的成员函数不修改成员变量的值"></a>修饰函数定义体，函数为类的成员函数，const修饰后的成员函数不修改成员变量的值</h2><p>define给一个立即数，const是常量，放在静态区域，全局变量也在静态区域<br>静态区：static无论是全局变量还是局部变量都存储在全局/静态区域，在编译期就为其分配内存，在程序结束时释放<br>const的全局变量存储在只读数据段，第一次使用时被分配内存，结束时释放；const的局部变量存在栈中，代码块结束释放<br>define定义的常量不可以用指针去指向，const定义的常量可以用指针去指向该常量的地址<br>–const优点<br>const 常量有数据类型，而宏常量没有数据类型。<br>编译器可以对前者进行类型安全检查，<br>后者只进行字符替换，没有类型安全检查，并且在字符替换可能报错。<br>[全局变量放在静态存储区，整个程序开始分配内存，结束释放]<br><strong>static</strong><br>static修饰的变量只能通过其所在文件、模块或函数进行调用，限制变量<br>static修饰的变量一开始就得初始化，并存放于静态内存区<br><strong>volatile</strong><br>本条指令不会因编译器的优化而省略，不会被编译器察觉（隐藏变量），且要求每次重新读取volatile修饰的变量的内容<br><strong>extern</strong></p><p><strong>指针和引用的区别</strong><br>引用本质是只读指针，引用只能在初始化时被赋值,且必须被初始化，之后不能改变，指针是动态的<br>引用不能为NULL，指针可以<br>引用做函数参数时，内部传递的是变量地址<br><strong>进程间通信</strong><br>pipe管道，半双工，用于父子进程通信<br>semaphore信号量，进程同步访问共享资源<br>message que 消息队列，克服了缓冲区限制<br>shared memory共享内存<br>socket<br><strong>线程间通信</strong><br>全局变量   Messages消息机制；<br>CEvent对象（MFC中的一种线程通信对象，通过其触发状态的改变实现同步与通信）</p><p>编译时运算符:sizeof</p><p><strong>写一个函数指针</strong><br><em>( ( void (</em>)() ) 0x100000) ( );<br>void(<em>)()强制转换0x100000<br>typedef void(</em>)() voidFunc;<br>*(  (voidFunc)0x100000 )();</p><p><strong>内存分配方式</strong></p><p>从静态存储区域分配。内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量。</p><p>在栈上创建。在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。</p><p>从堆上分配，亦称动态内存分配。程序在运行的时候用malloc或new申请任意多少的内存，程序员自己负责在何时用free或delete释放内存。动态内存的生存期由我们决定，使用非常灵活，但问题也最多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;持续更新&lt;/p&gt;
    
    </summary>
    
      <category term="CPP" scheme="http://waynamigo.github.io/categories/CPP/"/>
    
    
      <category term="CPP，面试" scheme="http://waynamigo.github.io/tags/CPP%EF%BC%8C%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>模型集成策略</title>
    <link href="http://waynamigo.github.io/2021/01/12/2021-01-12-%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/"/>
    <id>http://waynamigo.github.io/2021/01/12/2021-01-12-模型集成/</id>
    <published>2021-01-11T16:00:00.000Z</published>
    <updated>2023-01-01T12:01:31.183Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;stacking&quot;&gt;&lt;a href=&quot;#stacking&quot; class=&quot;headerlink&quot; title=&quot;stacking&quot;&gt;&lt;/a&gt;stacking&lt;/h2&gt;
      
    
    </summary>
    
      <category term="ML" scheme="http://waynamigo.github.io/categories/ML/"/>
    
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="DL" scheme="http://waynamigo.github.io/tags/DL/"/>
    
      <category term="stacking" scheme="http://waynamigo.github.io/tags/stacking/"/>
    
  </entry>
  
  <entry>
    <title>数据库系统相关</title>
    <link href="http://waynamigo.github.io/2021/01/11/2021-01-11-%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://waynamigo.github.io/2021/01/11/2021-01-11-数据库/</id>
    <published>2021-01-10T16:00:00.000Z</published>
    <updated>2022-07-18T12:28:36.718Z</updated>
    
    <content type="html"><![CDATA[<p>关于计算机研究生复试的数据库相关问题（笔试）</p><a id="more"></a><h2 id="系统概念相关"><a href="#系统概念相关" class="headerlink" title="系统概念相关"></a>系统概念相关</h2><p>数据视图<br>数据抽象： 物理层–&gt;逻辑层–&gt;视图层<br>实例和模式：物理模式–&gt;逻辑模式–&gt;子模式<br>数据集合是实例(Instance)， 数据库总体设计为数据库模式(Schema)<br>数据模型： 关系模型  实体-联系模型（E-R)  基于对象的模型  半结构化数据模型</p><h2 id="关系运算"><a href="#关系运算" class="headerlink" title="关系运算"></a>关系运算</h2><p>域：关系中的某属性允许取值的集合<br><strong>码</strong>：整个关系中区分不同元组的一种性质<br>超码 super key：一个或多个属性的集合，唯一标识一个元组,允许有多余的属性<br>候选码 candidate key：允许最少必要属性的超码即候选码比如{ID}{name,seat}是两个候选码<br>主码 primary key：设计者在一个关系内的候选码中选择的区分元组的属性组合<br>主码选择原则：选择那些值从不改变或极少改变的候选码作primary key<br>外码 foreign key：一个关系内的某属性是另一个关系的主码</p><p><strong>关系代数</strong></p><ul><li>选择元组/属性 σ</li><li>投影 π</li><li>自然连接 ∞</li><li>笛卡尔积 X</li><li>集合运算 交 并</li></ul><h2 id="自然连接举例"><a href="#自然连接举例" class="headerlink" title="自然连接举例"></a>自然连接举例</h2><table><thead><tr><th align="center">A</th><th align="center">B</th><th align="center">C</th><th></th><th align="center">D</th><th align="center">B</th><th align="center">E</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">a</td><td align="center">3</td><td></td><td align="center">2</td><td align="center">c</td><td align="center">7</td></tr><tr><td align="center">2</td><td align="center">b</td><td align="center">6</td><td></td><td align="center">3</td><td align="center">d</td><td align="center">5</td></tr><tr><td align="center">3</td><td align="center">c</td><td align="center">7</td><td></td><td align="center">1</td><td align="center">a</td><td align="center">3</td></tr><tr><td align="center">* 计算笛卡尔积</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">* 选出左B=右B的元组，不等的不算，忽略掉</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">* 合并该元组，成为新元组 A B C D E ，成为新元组的只有两组</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">1</td><td align="center">a</td><td align="center">3</td><td>1</td><td align="center">3</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">3</td><td align="center">c</td><td align="center">7</td><td>2</td><td align="center">7</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">## SQL相关</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">自然连接 nature join 和join using(某个属性)</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">并运算：union 自动去重 union all 可以保留重复</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">交运算：intersect 自动去重 intersect all 可保留重复</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">差运算：except 自动去重 except all 可保留重复</td><td align="center"></td><td align="center"></td><td></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>聚集函数：sum, min , max , count , avg<br>分组聚集： group by中没有出现的属性，只要是出现在select中，必须在聚集函数内部的形式出现,比如b,c没出现在group by 内部，用例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a,<span class="keyword">avg</span>(b),<span class="keyword">sum</span>(c) <span class="keyword">from</span> table1 <span class="keyword">group</span> <span class="keyword">by</span> a;</span><br></pre></td></tr></table></figure><p>集合成员资格：in ,not in<br>集合比较：some运算,用例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">ID</span> <span class="keyword">from</span> instructor </span><br><span class="line"><span class="keyword">where</span> salary &gt;<span class="keyword">some</span> (<span class="keyword">select</span> salary <span class="keyword">from</span> instructor </span><br><span class="line">    <span class="keyword">where</span> department = <span class="string">'bio'</span>);</span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> T.ID <span class="keyword">from</span> instructor <span class="keyword">as</span> T,instructor <span class="keyword">as</span> S </span><br><span class="line"><span class="keyword">where</span> S.department =<span class="string">'bio'</span> <span class="keyword">and</span> T.salary &gt;S.salary;</span><br></pre></td></tr></table></figure><p>空关系测试：exists, not exists,测试子查询结果中是否存在元组，用例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DSC黑书第六版中的，“找出选修了bio系开设的所有课程的学生”（表在官网</span><br><span class="line"><span class="keyword">select</span> S.ID,S.name <span class="keyword">from</span> student <span class="keyword">as</span> S </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">not</span> <span class="keyword">exists</span>( (<span class="keyword">select</span> course_id <span class="keyword">from</span> course</span><br><span class="line">   <span class="keyword">where</span> dep_name = <span class="string">'bio'</span>)//找出bio系开设的所有课程</span><br><span class="line">   <span class="keyword">except</span></span><br><span class="line">       (<span class="keyword">select</span> T.course_id <span class="keyword">from</span> takes <span class="keyword">as</span> T</span><br><span class="line">   <span class="keyword">where</span> S.ID = T.ID) );//找出S.ID选修的所有课程</span><br></pre></td></tr></table></figure><p>重复元组存在性测试：unique，测试子查询返回集合是否有重复元组，无则返回true；not unique则相反</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DSC黑书第六版中的，“找出所有在2019年最多开设一次的课程”</span><br><span class="line"><span class="keyword">select</span> C.course_id <span class="keyword">from</span> course <span class="keyword">as</span> C</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">unique</span> (<span class="keyword">select</span> S.course_id <span class="keyword">from</span> <span class="keyword">section</span> <span class="keyword">as</span> S</span><br><span class="line">      <span class="keyword">where</span> C.course_id =S.course_id  <span class="keyword">and</span> S.year =<span class="number">2019</span>);</span><br></pre></td></tr></table></figure><p>标量子查询：子查询只返回包括【单个属性】的【单个元组】，只可以出现在select where having三种子句中</p><h2 id="关系代数"><a href="#关系代数" class="headerlink" title="关系代数"></a>关系代数</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于计算机研究生复试的数据库相关问题（笔试）&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="http://waynamigo.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="笔试" scheme="http://waynamigo.github.io/tags/%E7%AC%94%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>统计学习笔记</title>
    <link href="http://waynamigo.github.io/2020/09/11/2020-09-12-statics_note/"/>
    <id>http://waynamigo.github.io/2020/09/11/2020-09-12-statics_note/</id>
    <published>2020-09-10T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:11.727Z</updated>
    
    <content type="html"><![CDATA[<p>该笔记是对李航统计学习方法和All of Statics做的学习笔记，简单进行相关算法实验，加强理解，查缺补漏等，内容尽量精炼</p><a id="more"></a><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><p>generative model和discriminative model$（以下分别表示为G和D）$<br>$G\ $常见的有朴素贝叶斯，隐马尔科夫模型，高斯混合、 LDA、 Restricted Boltzmann Machine等<br>$D\ $有Kmeans，感知机，决策树，最大熵模型，Logistic回归、SVM、 boosting、条件随机场、神经网络等<br>两者的本质区别及特点：<br>$G\ $的流程是<strong>学习X和Y的联合概率分布$P(x,y)$得出$P(y|x)$最直接的例子就是Naive Bayes</strong>，由于生成的结果是联合分布$P(x,y)$，可以计算边缘分布$P(x)$进行异常值检测，若$P(x)$太小，就判定可能不适合这一类样本所代表的数据。<br>$D\ $的流程是<strong>直接由给定的X，Y学习决策函数或$P(y|x)$，是一种黑盒操作，准确率高，可以将允许对问题进行抽象处理，最熟悉的例子就是Neural Network</strong></p><h3 id="分类问题和回归问题"><a href="#分类问题和回归问题" class="headerlink" title="分类问题和回归问题"></a>分类问题和回归问题</h3><p>分类用CrossEntropy，回归用Mean Square Error等等</p><h3 id="范数-norm"><a href="#范数-norm" class="headerlink" title="范数 norm"></a>范数 norm</h3><p>$L1范数 \sum{|x_i|}$<br>$L2范数 \sqrt{x_{1}^{2} + x_{2}^{2} + … + x_{n}^{2}}$<br>$L_\infty无穷范数MAX{|x_i|}$<br>范数理论推论$L1\geq{L2\geq{L_\infty}}$<br>对于numpy的线性代数库，有几种求范数的方法，主要就是求这三种</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.norm(x, ord=<span class="literal">None</span>, axis=<span class="literal">None</span>, keepdims=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>axis=0表示对矩阵x的每一列求范数，axis=1表示对矩阵的每一行求范数， keeptdims=True表示结果保留维度，keepdims=False表示结果不保留维度</p><h3 id="最小二乘"><a href="#最小二乘" class="headerlink" title="最小二乘"></a>最小二乘</h3><p>是解决曲线拟合问题、最小化cost的优化方法，使求得的数据与实际数据之间的误差平方和最小，应用范围非常广泛。<br>$设(x,y)为一组观测量，x=[x_0,x_1,…,x_n]^T,寻找一个函数y=f(x,w)$ ，使$尽可能逼近曲线(x,y),其中w=[w_0,w_1,…,w_n]^T$，为待估计参数，求解<br>使残差函数$$L(y,f(x,w))=\sum{[y_i-f(x_i,w_i)]^2}$$得到<strong>全局</strong>最小值的$w$,直观上就是每个点与拟合曲线的欧氏距离的平方和。</p><p><em>与梯度下降的区别：</em><br>最小二乘法是指对$\Delta$求导找出函数全局最小的w，梯度下降是先给定一个w（初始化），经过N次梯度下降后找到的使函数局部最小的w。相对的，梯度下降适用于大规模数据，最小二乘适用于较小样本，不过梯度下降的缺点是到最小点的时候收敛速度变、对初始点的选择极为敏感两个方面。</p><h2 id="感知机-perceptron"><a href="#感知机-perceptron" class="headerlink" title="感知机 perceptron"></a>感知机 perceptron</h2><p>属于$Discriminative \ Model$的线性分类模型，输入是表示一个Instance的特征向量，求出分离特征的超平面，公式表示为：<br>$f(x) = sign(w*x+b)$<br>$\begin{eqnarray}<br>sign(x)=  \begin{cases}<br>1,&amp;x\geq{0}  \cr<br>-1 ,&amp;x&lt;0<br>\end{cases}<br>\end{eqnarray}$<br>这种perceptron叠起来就相当于是全连接的MLP(Multi-Layer Perceptron)</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/MLP.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><p>n多个线性函数叠加，对应矩阵运算$W\cdot x + B$，$W是w权重矩阵，B是bias的列向量，激活函数对应单个感知机的sign函数$</p><h2 id="k-近邻-k-nearest-neighbor"><a href="#k-近邻-k-nearest-neighbor" class="headerlink" title="k-近邻 k nearest neighbor"></a>k-近邻 k nearest neighbor</h2><p>还是属于$Discriminative \ Model$的模型，复杂度为$O(n^2)$，由三个基本要素组成：<strong>距离度量、k值、分类规则</strong><br>距离度量，设有向量x1和x2，则：<br>欧氏距离<code>np.sqrt(np.sum(np.square(x1 - x2)))</code><br>或直接<code>np.linalg.norm(x1-x2)</code>（用numpy的线性代数库求L2范数，但后者较慢）<br>曼哈顿距离<code>np.sum(x1 - x2)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input:px,k</span><br><span class="line"><span class="keyword">return</span>:bestx</span><br><span class="line"><span class="comment"># get N(x):涵盖最近的k个点的邻域，即KList</span></span><br><span class="line">distList = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">distList.append(np.sqrt(np.sum(np.square(px - x))))</span><br><span class="line">KList = np.argsort(np.array(distList))[:k]</span><br><span class="line"><span class="comment"># 决策规则I:由KList得出bestx，以类别分类问题为例，选N(x)最多类别为结果</span></span><br><span class="line">X(np.argmax(np.bincount(X(i))))</span><br></pre></td></tr></table></figure><p>如果要求多个最大值索引<br><code>np.where(a == np.amax(a))[0]</code>，或者<code>np.argwhere(a == np.amax(a))</code></p><h3 id="kd-tree"><a href="#kd-tree" class="headerlink" title="kd tree"></a>kd tree</h3><p>存储k维空间数据的树结构，实现如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="朴素贝叶斯-Naive-Bayes"><a href="#朴素贝叶斯-Naive-Bayes" class="headerlink" title="朴素贝叶斯 Naive Bayes"></a>朴素贝叶斯 Naive Bayes</h2><p>属于$Generative \ Model$一类，给的是联合分布$P(x,y)$，学过概率论的应该都会，普通的算法实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">input:</span><br><span class="line">先验概率分布P__y : P(Y=c)，条件概率分布P_x_y : P(X=x|Y=c)，dim_f:特征维度</span><br><span class="line">c_num：分类数目，data:数据list，label:标签list，以[<span class="number">0</span>,<span class="number">1</span>,...,<span class="number">9</span>]为例</span><br><span class="line"><span class="keyword">return</span>:max P</span><br><span class="line"></span><br><span class="line"><span class="comment">#求出先验分布，并对数化，经常使用的对乘法处理的方式</span></span><br><span class="line">P__y = [[(np.sum(label == np.asarray(i)))/(len(label))] \</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(c_num)]</span><br><span class="line">P__y = np.log(P__y)</span><br><span class="line"><span class="comment">#求出条件分布</span></span><br><span class="line">P_x_y = np.zeros((c_num, dim_f, <span class="number">2</span>))  </span><br><span class="line"><span class="comment">#对标记集进行遍历  </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(label)):  </span><br><span class="line">    <span class="comment">#获取当前循环所使用的标记  </span></span><br><span class="line">c = label[i]  </span><br><span class="line">    <span class="comment">#获取当前要处理的样本</span></span><br><span class="line">x = data[i]  </span><br><span class="line">    <span class="comment">#对该样本的每一维feature进行遍历</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(dim_f):  </span><br><span class="line">        <span class="comment">#先在矩阵中对应位置加1</span></span><br><span class="line">P_x_y[c][j][x[j]] += <span class="number">1</span>  </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(c_num):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(dim_f):  </span><br><span class="line">P_x_y0 = P_x_y[c][j][<span class="number">0</span>]  </span><br><span class="line">P_x_y1 = P_x_y[c][j][<span class="number">1</span>]  </span><br><span class="line">P_x_y[c][j][<span class="number">0</span>] = np.log((P_x_y0 + <span class="number">1</span>) / (P_x_y0 + P_x_y1 + <span class="number">2</span>))</span><br><span class="line">P_x_y[c][j][<span class="number">1</span>] = np.log((P_x_y1 + <span class="number">1</span>) / (P_x_y0 + P_x_y1 + <span class="number">2</span>))</span><br><span class="line"><span class="comment"># pick up最大Probability</span></span><br><span class="line">P = [<span class="number">0</span>] * c_num</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(c_num):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(dim_f):</span><br><span class="line">sum += P_x_y[i][j][x[j]]  </span><br><span class="line">P[i] = sum + P__y[i] </span><br><span class="line">res = P.index(np.amax(P))</span><br></pre></td></tr></table></figure><h2 id="决策树-Decision-Tree-及剪枝"><a href="#决策树-Decision-Tree-及剪枝" class="headerlink" title="决策树 Decision Tree 及剪枝"></a>决策树 Decision Tree 及剪枝</h2><p>决策树是经常在kaggle以及实际应用中很广泛且有效的算法，决策树通常包括3个步骤:<strong>特征选择、构造、剪枝</strong>，<del>无内鬼，直接进行一个sklearn.tree的import</del>，sklearn的tree里封装了BaseDecisionTree，在此基础上进一步封装了DecisionTreeClassifier和DecisionTreeRegressor：分类器和回归器，做kaggle是确实好用。</p><h3 id="特征选择：特征选择的准则是信息增益（information-gain）或信息增益比。"><a href="#特征选择：特征选择的准则是信息增益（information-gain）或信息增益比。" class="headerlink" title="特征选择：特征选择的准则是信息增益（information gain）或信息增益比。"></a>特征选择：特征选择的准则是信息增益（information gain）或信息增益比。</h3><p>$设离散型X的概率分布P(X =x_i)=p_i$<br>$Entropy的定义为H(X)=\sum{p_i\log{p_i}}$</p><h3 id="决策树构造"><a href="#决策树构造" class="headerlink" title="决策树构造"></a>决策树构造</h3><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4><p>各个节点用信息增益H(D)准则选择特征，递归构建决策树。<br>ID3算法的核心是在决策树各个结点上用<strong>信息增益</strong>选择特征，递归地构建决策<br>树。具体方法是：从根结点（root node）开始，对结点计算所有可能的特征的信息增益， 选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归调用该方法，直到所有feature被用完或剩余feature的信息增益很小或少于自己设置的阈值，决策树建立完成，缺点是只生成了树，没有【】容易过拟合。</p><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4><p>各个节点用<strong>信息增益比</strong>选择特征，递归构建决策树，递归函数流程和ID3一样，只是评估标准换成了H(D|A)</p><h4 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h4><p>对回归树用<strong>平方最小误差</strong>原则，对分类树用<strong>基尼指数最小化</strong>原则进行特征选择。</p><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><p>但是自己还是得从0实现一个决策树，以后用的时候心里有点B数。<br>数据用colab的sampledata里california_housing那个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>熟悉的Logistic回归，以二分类任务为例，就是用sigmoid函数把结果映射到(-1,1)；多分类任务下，将该二分类任务的sigmoid推广到了softmax函数    ，就是我们熟悉的softmax激活函数。<br>$$Sigmoid(z) = \frac{1}{1+exp(-z)},z=w^T\cdot x,(alias\ Sigmoid(z)=h_w(x))$$<br>$$gradient\ descent:<br>\Delta = x_i \cdot y_i - \frac{np.exp(w\cdot x_i) * x_i)}{ ( 1 + np.exp(w\cdot x_i))}then, \ w=w+lr\cdot\Delta$$<br>或者<br>$$LikelihoodFunc:J(w) =-\frac{1}{m}\sum\limits_{i=1}^{m}{[y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))]}$$</p><p>$$partial:\frac{\partial J\left(w \right)}{\partial {w}}=\frac{1}{m}\sum\limits_{i=1}^{m}{(h_w(x_i)-{y_i})x_i}$$<br>代码例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#gradient descent</span></span><br><span class="line">x = data <span class="comment"># feature array,default(n,m), gradient dimension is m</span></span><br><span class="line">y = label <span class="comment"># result/ ground truth</span></span><br><span class="line">w = np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">iter_num = <span class="number">1000</span></span><br><span class="line">lr = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> one_iter <span class="keyword">in</span> range(iter_num):</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> len(data):</span><br><span class="line"><span class="comment"># 下面xiyi赋值是看着方便，实际上用的时候直接用index取list元素</span></span><br><span class="line">x_i = data[index]</span><br><span class="line">y_i = label[index]</span><br><span class="line"><span class="comment"># 用上面的公式，求partial</span></span><br><span class="line">gradient = x_i*(<span class="number">1</span>/(<span class="number">1</span>+np.exp(np.dot(w,x_i)))-y_i)</span><br><span class="line">w+=gradient*lr</span><br><span class="line">print(<span class="string">"final w:"</span>,w)</span><br></pre></td></tr></table></figure><h2 id="最大熵模型-Max-Entropy-Model"><a href="#最大熵模型-Max-Entropy-Model" class="headerlink" title="最大熵模型 Max Entropy Model"></a>最大熵模型 Max Entropy Model</h2><p><del>复杂度超高，做分类慢的一批</del>，一般用来衡量预测效果的好坏，<del>其实一般也不用</del>。主要是记录一下最大熵模型的思想：将分类等问题作为约束最优化问题，下面的SVM和Adaboost等算法都是采用的约束最优化思想完成的。</p><h2 id="支持向量机-Support-Vector-Machines"><a href="#支持向量机-Support-Vector-Machines" class="headerlink" title="支持向量机 Support Vector Machines"></a>支持向量机 Support Vector Machines</h2><p>间隔最大化的学习策略，可形式化为求解<strong>凸二次规划</strong>问题/正则化的合页损失函数的最小化问题<br>训练数据线性可分，通过硬间隔最大化（hard margin maximization）学习<em>线性可分SVM/硬间隔SVM</em><br>数据近似线性可分，通过软间隔最大化（soft margin maximization）学习<em>线性SVM/软间隔SVM</em><br>数据线性不可分时，通过核函数+软间隔最大化，学习<em>非线性SVM</em>：核函数表示将输入从输入空间映射到特征空间得到的特征向量的内积(点乘)，可以抽象成在高维空间里学习一个线性SVM</p><h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><p>函数间隔、约束最优化问题</p><ul><li>函数间隔：对于给定数据和超平面wx+b：<br>关于样本点(x,y)的函数间隔为$\gamma_f=y(wx+b)$<br>关于数据集的函数间隔为，所有样本点的最小值，$\gamma_{min}=min(\gamma_f)$</li><li>几何间隔：归一化函数间隔，在法向量正向的几何间隔为$\gamma_g=y(\frac{w}{||w||}\cdot{x}+\frac{b}{||w||}),其中||w||是法向量w的L_2范数$<br>两者关系是$\gamma_f=\gamma_g*||w||$</li><li>间隔最大化，我们为使SVM分类样本点的置信度更大，需要将超平面关于数据集的几何间隔最大化，即求<em>最大几何间隔的超平面</em>，数学描述为：<br>$$max\ \frac{\gamma_f}{||w||}\\ s.t.\ y(wx+b)\geq \gamma_{min}$$<br>由于等式两边在尺度上是一致的，用一下无敌的“不妨设”$\gamma_f = 1$，那么优化目标为w的L2范数的最小值，即<br>$$max\ \frac{\gamma_f}{||w||}等价于求\min{\frac{||w||^2}{2}}\\ s.t.\ y(wx+b)\geq{1}$$<br>那么这个转化为二次规划的非线性规划如何求解呢？<br>使用<strong>拉格朗日对偶性</strong>求解对偶问题得到以上问题的解，以这个线性可分问题为例，引入N个拉格朗日乘子，$\alpha$，对应N维特征和N维法向量w：<br>$$构建拉格朗日函数L(w,b,\alpha)=\frac{||w||^2}{2}-\sum\limits_{i=1}^N{\alpha_iy_i(wx_i+b)}+\sum\limits_{i=1}^N{\alpha_i}$$<br>原始问题的对偶问题转化为$\max\limits_{\alpha}\min\limits_{w,b}L,下面推导一下$<br>Derivatives:<br>$$\frac{\partial{L(w,b,\alpha)}}{\partial{w}}=w-\sum\limits_{i=1}^N{\alpha_ix_iy_i}=0\\ \frac{\partial{L(w,b,\alpha)}}{\partial{b}}=\sum\limits_{i=1}^N{\alpha_iy_i}=0$$<br>Then we turn to:<br>$$max:L(w,b,\alpha)=\sum\limits_{i=1}^N{\alpha_i}-\frac{1}{2}\sum\limits_{i,j=1}^N{y_iy_j\alpha_i\alpha_jx_i^Tx_j}\\ s.t\ \sum\limits_{i=1}^N{\alpha_iy_i}=0$$</li></ul><p>这化简为只有拉格朗日乘子alpha的L极大值问题了，到这一步，我们可以直接进行SMO求解（从这里可以直接跳到下一节）<br>于是我们可以引入软间隔的线性SVM，对每个样本点引进一个松弛变量$\xi\geq0$，再引进一个惩罚参数C，那么我们的问题由$求min\frac{||w||^2}{2}转化为min(\frac{||w||^2}{2}+C\sum\limits_{i=1}^N{\xi_i})$<br>$$L(w,b,\xi,\alpha,\mu)=\frac{||w||^2}{2}+C\sum\limits_{i=1}^N{\xi_i}-\sum\limits_{i=1}^N{\alpha_iy_i(wx_i+b)}+\sum\limits_{i=1}^N{\alpha_i(1-\xi_i)}-\sum\limits_{i=1}^N{\mu_i\xi_i},\\<br>s.t.\ y(wx+b)\geq1-\xi,\xi\geq0$$<br>Derivatives:<br>$$\frac{\partial{L}}{\partial{w}}=w-\sum\limits_{i=1}^N{\alpha_ix_iy_i}=0\\<br>\frac{\partial{L}}{\partial{b}}=-\sum\limits_{i=1}^N{\alpha_iy_i}=0\\<br>\frac{\partial{L}}{\partial{\xi_i}}=C-\alpha_i-\mu_i=0$$<br>以上求出关于$w,b,\xi$的极小后<br>turn to :<br>$$max:L(w,b,\xi,\alpha,\mu)=\sum\limits_{i=1}^N{\alpha_i}-\frac{1}{2}\sum\limits_{i,j=1}^N{y_iy_j\alpha_i\alpha_jx_i^Tx_j}\\<br>s.t.\ \sum\limits_{i=1}^N{\alpha_iy_i}=0,\\<br>C-\alpha_i-\mu_i=0$$<br>由以上结果可以看出，如果将目标函数的max转化为求min(改正负号)，均得到对应的对偶问题，其满足KKT条件，经过求解对偶问题，得出alpha，带入解得w和b，$$w=\sum\limits_{i=1}^N{\alpha_ix_iy_i}\\<br>b=y_j-\sum\limits_{i=1}^N{}y_i\alpha_i(x_ix_j)$$即得到超平面，<strong>wx+b=0</strong><br>以上两种线性的SVM可以直接由上面的推导将一个求最大间隔的原始问题转化为求一个超平面的对偶问题，进而求得</p><h3 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h3><p>核函数用来将两个样本点实例$x,z$通过映射函数$\Phi(x),\Phi(z)$从输入空间映射到特征空间内，核函数表示为K，即$K(x,z)=\Phi(x)^T\Phi(z)$，一般不写出映射函数$\Phi$，而是在Kernel函数中隐式给出：<br>在这记录一下高斯核Gaussian kernel(radial basis function,RBF kernel):<br>$$K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})$$和sigmoid核：<br>$$K(x,z)=tanh(ax^Tz+c)\\ tanh(b)=\frac{1-e^{-2b}}{1+e^{-2b}}$$<br>“SVM with a sigmoid kernel is equivalent to a 2-layer perceptron”，一个结论，显式的证明就不用写了，其实在看到拉格朗日乘子alpha时，我们就可以直观的联想到拉格朗日乘子相当于感知机场景下对feature的权重。</p><h3 id="序列最小最优化算法，sequential-minimal-optimization-SMO-alg"><a href="#序列最小最优化算法，sequential-minimal-optimization-SMO-alg" class="headerlink" title="序列最小最优化算法，sequential minimal optimization,SMO alg."></a>序列最小最优化算法，sequential minimal optimization,SMO alg.</h3><p>引入核函数的非线性转化为线性（甚至是可分）的凸二次规划问题：<br>$$\min\limits_{\alpha}:\frac{1}{2}\sum\limits_{i,j=1}^N{y_iy_j\alpha_i\alpha_jx_i^Tx_j}-\sum\limits_{i=1}^N{\alpha_i},\\<br>s.t.\ \sum\limits_{i=1}^N{\alpha_iy_i}=0$$<br>非线性引入Gaussian核的SVM实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def SVM():</span><br></pre></td></tr></table></figure><h3 id="概念补充"><a href="#概念补充" class="headerlink" title="概念补充"></a>概念补充</h3><p><em>supprot vector</em>:线性不可分情况下，对偶问题的解$\alpha=(a_1,a_2…a_N)^T中a_i对应的样本点(x_i,y_i)就是支持向量。$<br><em>凸优化问题</em>：设$f:F\rightarrow{R}为$凸函数，则求$\min\limits_{x\in{F}}{f(x)}为$凸优化问题<br>凸优化有如下几个定理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">凸优化任意局部最优解即全局最优解</span><br><span class="line">凸优化最优解集为凸集</span><br><span class="line">若函数f为非空凸集上的严格凸函数，且凸优化问题存在全局最优解，那么全局最优解唯一</span><br></pre></td></tr></table></figure><p>在条件$f_i(x)\leq0,1,a_i^T\cdot x = b_i$最小化$f_0(x)$，<br>凸集指一个集合空间内部两点间连线所覆盖的点都在集合空间内，<br>凸二次规划（convex quadratic programming）指目标函数为凸二次函数，形如<br>$$min f(x)= \frac{1}{2}x^TQx+C^Tx,\\<br>s.t.\ Ax\leq{b}，其每一行对应一个约束$$<br><em>Karush-Kuhn-Tucker condition:</em><br>$\alpha_i\geq{0}\\<br>y_i(wx_i+b)\geq{1}\\<br>\alpha_i(y_i(w_ix+b)-1)=0$</p><h2 id="随机森林，梯度提升决策树"><a href="#随机森林，梯度提升决策树" class="headerlink" title="随机森林，梯度提升决策树"></a>随机森林，梯度提升决策树</h2><p>梯度提升决策树（GBDT）对于输入的一个样本实例，首先会赋予一个初值，然后会遍历每一棵决策树，每棵树都会对预测值进行调整修正，最后得到预测的结果</p><p>随机森林减少模型方差，提高性能<br>GBDT减少模型偏差，提高性能</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该笔记是对李航统计学习方法和All of Statics做的学习笔记，简单进行相关算法实验，加强理解，查缺补漏等，内容尽量精炼&lt;/p&gt;
    
    </summary>
    
      <category term="数学" scheme="http://waynamigo.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="数学" scheme="http://waynamigo.github.io/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>云计算技术栈学习路线</title>
    <link href="http://waynamigo.github.io/2020/01/04/2020-01-04-%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E6%A0%88/"/>
    <id>http://waynamigo.github.io/2020/01/04/2020-01-04-云计算技术栈/</id>
    <published>2020-01-03T16:00:00.000Z</published>
    <updated>2023-01-04T06:27:40.419Z</updated>
    
    <content type="html"><![CDATA[<p>，忘记从哪复制过来的了，侵删（</p><a id="more"></a><h1 id="Google-三件套"><a href="#Google-三件套" class="headerlink" title="Google 三件套"></a>Google 三件套</h1><p>Hadoop对应于Google三件套<br>HDFS对应于GFS，即分布式文件系统，MapReduce即并行计算框架，HBase对应于BigTable，即分布式NoSQL列数据库，外加Zookeeper对应于Chubby，即分布式锁设施。</p><p>hadoop -&gt;<br>zookeeper -&gt;<br>hive -&gt;<br>flume &amp;&amp; sqoop -&gt;<br>azkaban &amp;&amp; oozie -&gt;<br>数仓建模理论+实践（离线数仓项目） -&gt;<br>hbase -&gt;<br>redis -&gt;<br>kafka -&gt;<br>elk -&gt;<br>scala -&gt;<br>spark -&gt;<br>kylin -&gt;<br>flink -&gt;<br>实时数仓项目</p><p>Scala底层也是使用的JVM虚拟机</p><h1 id="Hadoop（重点）"><a href="#Hadoop（重点）" class="headerlink" title="Hadoop（重点）"></a>Hadoop（重点）</h1><p>学习Hadoop，需要重点掌握Hadoop的三个组件：MapReduce、HDFS、YarnZookeeper（会用，懂原理）</p><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>意为动物园管理者，是一个分布式应用程序协调框架，负责协调大数据框架的</p><h1 id="Hive（重点）"><a href="#Hive（重点）" class="headerlink" title="Hive（重点）"></a>Hive（重点）</h1><p>Hive底层依赖Hadoop，所以学完Hadoop在学Hive很简单，因为Hive是数仓工具，使用SQL开发的，如果懂SQL语句，那Hive学起来更简单了</p><h1 id="Flume（会用）"><a href="#Flume（会用）" class="headerlink" title="Flume（会用）"></a>Flume（会用）</h1><p>Flume就是一个采集工具，比如把日志实时采集到大数据平台上，用Flume即可</p><h1 id="Sqoop（会用）"><a href="#Sqoop（会用）" class="headerlink" title="Sqoop（会用）"></a>Sqoop（会用）</h1><p>Sqoop也是采集工具，但是和Flume定位不同，Sqoop是hadoop和其他数据库之间移动数据Flume是从各种来源收集数据，例如日志，jms，目录等</p><h1 id="azkaban和oozie（会用）"><a href="#azkaban和oozie（会用）" class="headerlink" title="azkaban和oozie（会用）"></a>azkaban和oozie（会用）</h1><p>这两个框架属于一类，都是资源调度框架，比如每天定时跑一些大数据的任务，就可以在这上面操作，这两个框架区别就是azkaban功能简单，易上手，oozie功能多，上手相对复杂一点</p><h1 id="数仓理论-实践（重点）"><a href="#数仓理论-实践（重点）" class="headerlink" title="数仓理论+实践（重点）"></a>数仓理论+实践（重点）</h1><p>学到这可以做一些项目了，找一些离线数仓的项目做下，做项目的同时需要理解数仓建模的理论，数仓为什么这样建，有什么好处，以后可能会出现什么隐患等，需要重点关注</p><h1 id="HBase（重点）"><a href="#HBase（重点）" class="headerlink" title="HBase（重点）"></a>HBase（重点）</h1><p>HBase是一个分布式列式数据库，适合存储海量的数据，能进行秒级查询，需要重点学习</p><h1 id="Kafka（重点）"><a href="#Kafka（重点）" class="headerlink" title="Kafka（重点）"></a>Kafka（重点）</h1><p>Kafka是大数据消息队列领域唯一的王者，不但工作常用，面试也常问，需要理解底层原理ELK（会用，最好也深入下）</p><h1 id="ELK是三个组件的简称，"><a href="#ELK是三个组件的简称，" class="headerlink" title="ELK是三个组件的简称，"></a>ELK是三个组件的简称，</h1><p>它们是Elasticsearch、Logstash、Kibana，Elasticsearch 是一个基于 Lucene 的、支持全文索引的分布式存储和索引引擎；Logstash是一个日志收集、过滤、转发的中间件；Kibana是一个可视化工具，主要负责查询 Elasticsearch 的数据并以可视化的方式展现给业务方</p><h1 id="Scala（重点）"><a href="#Scala（重点）" class="headerlink" title="Scala（重点）"></a>Scala（重点）</h1><p>前面也说了，这是大数据必学的一门语言，因为Spark和Flink底层都是基于Scala开发的，当然也有部门Java开发的</p><h1 id="Spark（重点）"><a href="#Spark（重点）" class="headerlink" title="Spark（重点）"></a>Spark（重点）</h1><p>Spark 是专为大规模数据处理而设计的快速通用的计算引擎，支持批处理和流处理，目前主要用在批处理领域</p><h1 id="Kylin（会用）"><a href="#Kylin（会用）" class="headerlink" title="Kylin（会用）"></a>Kylin（会用）</h1><p>Kylin的出现就是为了解决大数据系统中TB级别数据的数据分析需求，它提供Hadoop/Spark之上的SQL查询接口及多维分析(OLAP)能力以支持超大规模数据，它能在亚秒内查询巨大的Hive表。其核心是预计算，计算结果存在HBase中</p><h1 id="Flink（重点）"><a href="#Flink（重点）" class="headerlink" title="Flink（重点）"></a>Flink（重点）</h1><p>Flink目前非常火，和Spark一样，支持批处理和流处理，目前主要用在流处理实时数仓项目（重点）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;，忘记从哪复制过来的了，侵删（&lt;/p&gt;
    
    </summary>
    
      <category term="CloudComputing" scheme="http://waynamigo.github.io/categories/CloudComputing/"/>
    
    
      <category term="CloudComputing" scheme="http://waynamigo.github.io/tags/CloudComputing/"/>
    
  </entry>
  
  <entry>
    <title>Solidity编写smart contract的demo</title>
    <link href="http://waynamigo.github.io/2019/07/22/2019-07-22-blockchain/"/>
    <id>http://waynamigo.github.io/2019/07/22/2019-07-22-blockchain/</id>
    <published>2019-07-21T16:00:00.000Z</published>
    <updated>2022-07-16T04:45:11.372Z</updated>
    
    <content type="html"><![CDATA[<p>暑假开始的区块链+深度学习的小项目，关于写smart contract的阶段性记录(持续更新)<br>ps:清收藏夹时发现的奇异AI社区，地址失效了，现在是<a href="http://talk.strangeai.pro" target="_blank" rel="noopener">http://talk.strangeai.pro</a><br>（早期是将人工智能算法以平台的形式提供给普通开发者，让开发者来贡献、提交开源或者自有的算法。现在名字改成ManaAI了，开放的算法代码也下架了,遗憾）</p><a id="more"></a><p><a href="http://ethdoc.cn/" target="_blank" rel="noopener">eth文档</a><br><a href="https://solidity-cn.readthedocs.io" target="_blank" rel="noopener">solidity文档</a></p><h2 id="测试网络Rinkeby"><a href="#测试网络Rinkeby" class="headerlink" title="测试网络Rinkeby"></a>测试网络Rinkeby</h2><p>Rinkeby是以太坊官方提供的测试网络，使用PoA共识机制<br>PoA流程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">创世块中指定一组初始授权的signers,所有地址保存在创世区块(Genesis Block),并且把该区块的hash写到钱包里。</span><br><span class="line"></span><br><span class="line">启动挖矿后, 该组signers开始对生成的block进行签名并广播</span><br><span class="line"></span><br><span class="line">签名结果保存在区块头的Extra字段中</span><br><span class="line"></span><br><span class="line">Extra中更新当前高度已授权的所有signers的地址,因为有新加入或踢出的signer</span><br><span class="line"></span><br><span class="line">每一高度都有一个signer处于IN-TURN状态, 其他signer处于OUT-OF-TURN状态, IN-TURN的signer签名的block会立即广播, OUT-OF-TURN的signer签名的block会延时一段时间后再广播, 保证IN-TURN的签名block有更高的优先级上链</span><br><span class="line"></span><br><span class="line">如果需要加入一个新的signer,signer通过API接口发起一个proposal, 该proposal通过复用区块头 Coinbase(新signer地址)和Nonce(&quot;0xffffffffffffffff&quot;) 字段广播给其他节点. 所有已授权的signers对该新的signer进行&quot;加入&quot;投票, 如果赞成票超过signers总数的50%, 表示同意加入</span><br><span class="line"></span><br><span class="line">如果需要踢出一个旧的signer, 所有已授权的signers对该旧的signer进行&quot;踢出&quot;投票, 如果赞成票超过signers总数的50%, 表示同意踢出</span><br></pre></td></tr></table></figure><h2 id="Solidity"><a href="#Solidity" class="headerlink" title="Solidity"></a>Solidity</h2><p>code<br>请稍等</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;暑假开始的区块链+深度学习的小项目，关于写smart contract的阶段性记录(持续更新)&lt;br&gt;ps:清收藏夹时发现的奇异AI社区，地址失效了，现在是&lt;a href=&quot;http://talk.strangeai.pro&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://talk.strangeai.pro&lt;/a&gt;&lt;br&gt;（早期是将人工智能算法以平台的形式提供给普通开发者，让开发者来贡献、提交开源或者自有的算法。现在名字改成ManaAI了，开放的算法代码也下架了,遗憾）&lt;/p&gt;
    
    </summary>
    
      <category term="Solidity" scheme="http://waynamigo.github.io/categories/Solidity/"/>
    
    
      <category term="paper" scheme="http://waynamigo.github.io/tags/paper/"/>
    
      <category term="ML" scheme="http://waynamigo.github.io/tags/ML/"/>
    
      <category term="blockchain" scheme="http://waynamigo.github.io/tags/blockchain/"/>
    
      <category term="Solidity" scheme="http://waynamigo.github.io/tags/Solidity/"/>
    
  </entry>
  
</feed>
